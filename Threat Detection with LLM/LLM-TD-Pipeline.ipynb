{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e2a927",
   "metadata": {},
   "source": [
    "# LLM powered Threat Detection Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beef23ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Part 1 - IOC Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af69b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Setup\n",
    "#!pip install openai\n",
    "import os, re, json, csv, gzip, glob, hashlib, base64, ipaddress, requests, urllib3, time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Iterable\n",
    "import pandas as pd\n",
    "import tldextract\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "try:\n",
    "    import ujson as json_fast\n",
    "except Exception:\n",
    "    json_fast = json\n",
    "\n",
    "# OpenAI api key for enrichment, optional\n",
    "OPENAI_API_KEY = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee097d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: C:\\Users\\ronal\\Desktop\\TDLab\\data_inputs\n",
      "EVE_DIR: C:\\Users\\ronal\\Desktop\\TDLab\\eve_inputs\n",
      "OUT_DIR: C:\\Users\\ronal\\Desktop\\TDLab\\generated\n"
     ]
    }
   ],
   "source": [
    "#2. Configuration\n",
    "# input data folder, Suricata EVE json file input folder, and results output folder\n",
    "DATA_DIR = Path(\"./data_inputs\")\n",
    "EVE_DIR = Path(\"./eve_inputs\")\n",
    "OUT_DIR = Path(\"./generated\")\n",
    "OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"DATA_DIR:\", DATA_DIR.resolve())\n",
    "print(\"EVE_DIR:\", EVE_DIR.resolve())\n",
    "print(\"OUT_DIR:\", OUT_DIR.resolve())\n",
    "\n",
    "# MISP url and key to push IOCs to MISP\n",
    "MISP_URL = \"https://localhost\"\n",
    "MISP_KEY = \"YOUR_MISP_API_KEY\"\n",
    "VERIFY_SSL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdf092fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 text files from data_inputs.\n",
      "Extracted 381 IOCs -> generated\\iocs_extracted.csv.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>type</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11 Malicious Go Packages Distribute Obfuscated...</td>\n",
       "      <td>sha1</td>\n",
       "      <td>ec8bdee1c73de2b3488646aa1b77316664ea7751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11 Malicious Go Packages Distribute Obfuscated...</td>\n",
       "      <td>sha1</td>\n",
       "      <td>ae2ae799b2e9ba834168d41a5d659d2cc61558c5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11 Malicious Go Packages Distribute Obfuscated...</td>\n",
       "      <td>sha1</td>\n",
       "      <td>86b95cad5f9b483f702c5406dc9d819a860e61e7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11 Malicious Go Packages Distribute Obfuscated...</td>\n",
       "      <td>sha1</td>\n",
       "      <td>2486c036a322a5b0d9d09c64a690892d85347dee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11 Malicious Go Packages Distribute Obfuscated...</td>\n",
       "      <td>sha1</td>\n",
       "      <td>a71515277b747364bc1385d01c6027bc60f66a15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11 Malicious Go Packages Distribute Obfuscated...</td>\n",
       "      <td>sha1</td>\n",
       "      <td>e18d6a3f4746e0600888926a2b97a2b8c3f0a29c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11 Malicious Go Packages Distribute Obfuscated...</td>\n",
       "      <td>sha1</td>\n",
       "      <td>7cb4122d637d8b6c4c4dc6427832b5c1ba993f0b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11 Malicious Go Packages Distribute Obfuscated...</td>\n",
       "      <td>sha1</td>\n",
       "      <td>477f38004f3c5b13398ff6abe718e925af36ae0b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11 Malicious Go Packages Distribute Obfuscated...</td>\n",
       "      <td>sha1</td>\n",
       "      <td>07372dc1581f6b0fed6246998585bccd6d3d4e5c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11 Malicious Go Packages Distribute Obfuscated...</td>\n",
       "      <td>sha1</td>\n",
       "      <td>ff1c0dd60d9d708ad800064f6724d3d541a141ce</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  type  \\\n",
       "0  11 Malicious Go Packages Distribute Obfuscated...  sha1   \n",
       "1  11 Malicious Go Packages Distribute Obfuscated...  sha1   \n",
       "2  11 Malicious Go Packages Distribute Obfuscated...  sha1   \n",
       "3  11 Malicious Go Packages Distribute Obfuscated...  sha1   \n",
       "4  11 Malicious Go Packages Distribute Obfuscated...  sha1   \n",
       "5  11 Malicious Go Packages Distribute Obfuscated...  sha1   \n",
       "6  11 Malicious Go Packages Distribute Obfuscated...  sha1   \n",
       "7  11 Malicious Go Packages Distribute Obfuscated...  sha1   \n",
       "8  11 Malicious Go Packages Distribute Obfuscated...  sha1   \n",
       "9  11 Malicious Go Packages Distribute Obfuscated...  sha1   \n",
       "\n",
       "                                      value  \n",
       "0  ec8bdee1c73de2b3488646aa1b77316664ea7751  \n",
       "1  ae2ae799b2e9ba834168d41a5d659d2cc61558c5  \n",
       "2  86b95cad5f9b483f702c5406dc9d819a860e61e7  \n",
       "3  2486c036a322a5b0d9d09c64a690892d85347dee  \n",
       "4  a71515277b747364bc1385d01c6027bc60f66a15  \n",
       "5  e18d6a3f4746e0600888926a2b97a2b8c3f0a29c  \n",
       "6  7cb4122d637d8b6c4c4dc6427832b5c1ba993f0b  \n",
       "7  477f38004f3c5b13398ff6abe718e925af36ae0b  \n",
       "8  07372dc1581f6b0fed6246998585bccd6d3d4e5c  \n",
       "9  ff1c0dd60d9d708ad800064f6724d3d541a141ce  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. IOC Extraction\n",
    "IOC_REGEX = {\n",
    "    \"ipv4\": re.compile(r\"\\b(?:(?:25[0-5]|2[0-4]\\d|1?\\d?\\d)\\.){3}(?:25[0-5]|2[0-4]\\d|1?\\d?\\d)\\b\"),\n",
    "    \"md5\": re.compile(r\"\\b[a-fA-F0-9]{32}\\b\"),\n",
    "    \"sha1\": re.compile(r\"\\b[a-fA-F0-9]{40}\\b\"),\n",
    "    \"sha256\": re.compile(r\"\\b[a-fA-F0-9]{64}\\b\"),\n",
    "    \"url\": re.compile(r\"\\bhttps?://[\\w\\-\\.\\/:\\?#\\[\\]@!$&'()*+,;=%]+\", re.IGNORECASE),\n",
    "    \"email\": re.compile(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\"),\n",
    "}\n",
    "\n",
    "def is_domain(token: str) -> bool:\n",
    "    ext = tldextract.extract(token)     # split a string into domain, subdomain, and suffix by using PSL(Public Suffix List)-aware validation\n",
    "    return bool(ext.domain and ext.suffix)  # true if there is a second level domain and a known public suffix\n",
    "\n",
    "def extract_domains(text: str):\n",
    "    candidates = set()\n",
    "    for token in re.findall(r\"[\\w.-]+\\.[a-zA-Z]{2,}\", text):    # find dotty tokens look like domain\n",
    "        if token.lower().startswith(\"http\"):        # exclude http string which already has been counted\n",
    "            continue\n",
    "        if is_domain(token):        # to keep only strings that tldextract recognizes as a registable domain\n",
    "            candidates.add(token.lower())\n",
    "    return sorted(candidates)\n",
    "\n",
    "def extract_iocs_from_text(text: str):\n",
    "    out = {k: [] for k in [\"ipv4\", \"md5\", \"sha1\", \"sha256\", \"url\", \"email\",\"domain\"]}   # prepare an output dict for each IOC type\n",
    "    for k, rx in IOC_REGEX.items():\n",
    "        if k == \"url\":\n",
    "            out[k] = list({m.group(0).strip(').,;\"\\'') for m in rx.finditer(text)}) # trim common trailing punctuation like ).,;\"' that often clings to links in prose.\n",
    "        else:\n",
    "            out[k] = list({m.group(0) for m in rx.finditer(text)})\n",
    "    out[\"domain\"] = extract_domains(text)\n",
    "    return out\n",
    "\n",
    "def load_texts_from_dir(path: Path):\n",
    "    texts = {}\n",
    "    for p in path.glob(\"**/*\"):\n",
    "        if p.is_file() and p.suffix.lower() in {\".txt\",\".md\",\".html\",\".htm\",\".log\"}:    # load input files with these extensions\n",
    "            try:\n",
    "                texts[p.name] = p.read_text(errors=\"ignore\")\n",
    "            except Exception:\n",
    "                pass\n",
    "    return texts\n",
    "\n",
    "texts = load_texts_from_dir(DATA_DIR)\n",
    "print(f\"Loaded {len(texts)} text files from {DATA_DIR}.\")\n",
    "\n",
    "rows = []\n",
    "for fname, content in texts.items():\n",
    "    iocs = extract_iocs_from_text(content)\n",
    "    for t, values in iocs.items():\n",
    "        for v in values:\n",
    "            rows.append({\"source\": fname, \"type\": t, \"value\": v})\n",
    "\n",
    "ioc_df = pd.DataFrame(rows).drop_duplicates().reset_index(drop=True)\n",
    "ioc_csv = OUT_DIR / \"iocs_extracted.csv\"\n",
    "ioc_df.to_csv(ioc_csv, index=False)\n",
    "print(f\"Extracted {len(ioc_df)} IOCs -> {ioc_csv}.\")\n",
    "ioc_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2daa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planned lookups this run: 250 (from offset 0)\n",
      "[checkpoint] where 100 rows to generated\\vt_reputation.csv.\n",
      "[checkpoint] where 125 rows to generated\\vt_reputation.csv.\n",
      "[checkpoint] where 150 rows to generated\\vt_reputation.csv.\n",
      "[checkpoint] where 175 rows to generated\\vt_reputation.csv.\n",
      "[checkpoint] where 200 rows to generated\\vt_reputation.csv.\n",
      "[checkpoint] where 225 rows to generated\\vt_reputation.csv.\n",
      "[checkpoint] where 250 rows to generated\\vt_reputation.csv.\n",
      "[checkpoint] where 275 rows to generated\\vt_reputation.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronal\\AppData\\Local\\Temp\\ipykernel_26544\\2679749991.py:151: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out_df = pd.concat([out_df, tmp], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[checkpoint] where 300 rows to generated\\vt_reputation.csv.\n",
      "Done. Total rows in generated\\vt_reputation.csv: 304.\n",
      "Flagged subset -> generated\\vt_flagged.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronal\\AppData\\Local\\Temp\\ipykernel_26544\\2679749991.py:160: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out_df = pd.concat([out_df, tmp], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "#4. check extracted IOCs reputation in VirusTotal\n",
    "import requests, time\n",
    "\n",
    "VT_API_KEY = \"YOUT_VT_API_KEY\"\n",
    "RATE_LIMIT_QPM = int(\"100\")    # queries per minute; free VT API keys are low.\n",
    "SLEEP_SEC = max(60.0 / max(1, RATE_LIMIT_QPM), 10.0)\n",
    "CHUNK_CHECKPOINT = 25   # write csv checkpoint after this many lookups\n",
    "START_OFFSET = int(\"0\")\n",
    "MAX_ITEMS = int(\"250\")  # process at most this many items this run\n",
    "VT_ONLY_NEW = True      # skip items already present in vt_reputation.csv file\n",
    "\n",
    "IOC_CSV = OUT_DIR / \"iocs_extracted.csv\"    # input\n",
    "OUT_CSV = OUT_DIR / \"vt_reputation.csv\"     # output\n",
    "\n",
    "# load IOCs from in-memory df if present, else from csv file\n",
    "if \"ioc_df\" in globals() and isinstance(ioc_df, pd.DataFrame) and not ioc_df.empty:\n",
    "    src_df = ioc_df.copy()\n",
    "else:\n",
    "    if not IOC_CSV.exists():\n",
    "        raise FileNotFoundError(f\"IOC csv not found: {IOC_CSV}. Run Part 1 script to generate it first.\")\n",
    "    src_df = pd.read_csv(IOC_CSV)\n",
    "\n",
    "# canonicalize and split\n",
    "def get_values(df, t):\n",
    "    return sorted(set(df.loc[df[\"type\"] == t, \"value\"].dropna().astype(str)))\n",
    "\n",
    "ips = get_values(src_df, \"ipv4\")\n",
    "domains = get_values(src_df, \"domain\")\n",
    "urls = get_values(src_df, \"url\")\n",
    "hashes = sorted(set(pd.concat([\n",
    "    src_df.loc[src_df[\"type\"] == \"md5\", \"value\"],\n",
    "    src_df.loc[src_df[\"type\"] == \"sha1\", \"value\"],\n",
    "    src_df.loc[src_df[\"type\"] == \"sha256\", \"value\"],\n",
    "], axis=0).dropna().astype(str)))\n",
    "\n",
    "def vt_headers():\n",
    "    return {\"x-apikey\": VT_API_KEY}\n",
    "\n",
    "def vt_url_for(kind, value):\n",
    "    if kind == \"ip\":\n",
    "        return f\"https://www.virustotal.com/api/v3/ip_addresses/{value}\"\n",
    "    if kind == \"domain\":\n",
    "        return f\"https://www.virustotal.com/api/v3/domains/{value}\"\n",
    "    if kind == \"url\":\n",
    "        url_id = base64.urlsafe_b64encode(value.encode()).decode().strip(\"=\")   # url_id is urlsafe base64 of the url, without padding\n",
    "        return f\"https://www.virustotal.com/api/v3/urls/{url_id}\"\n",
    "    if kind == \"file\":\n",
    "        return f\"https://www.virustotal.com/api/v3/files/{value}\"\n",
    "    raise ValueError(kind)\n",
    "\n",
    "def classify_kind(value, explicit_type=None):\n",
    "    if explicit_type:\n",
    "        return explicit_type\n",
    "    try:\n",
    "        ipaddress.IPv4Address(value)\n",
    "        return \"ip\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    if value.startswith(\"http://\") or value.startswith(\"https://\"):\n",
    "        return \"url\"\n",
    "    if len(value) in (32, 40, 64) and all(c in \"0123456789abcdefABCDEF\" for c in value):\n",
    "        return \"file\"\n",
    "    return \"domain\"\n",
    "\n",
    "def parse_stats(kind, vt_json):\n",
    "    stats = {\"malicious\": None, \"suspicious\": None, \"undetected\": None, \"harmless\": None}\n",
    "    rep = None\n",
    "    last_date = None\n",
    "    try:\n",
    "        attr = vt_json.get(\"data\", {}).get(\"attributes\", {})\n",
    "        s = attr.get(\"last_analysis_stats\") or {}\n",
    "        stats.update({k: s.get(k) for k in stats.keys()})\n",
    "        rep = attr.get(\"reputation\")\n",
    "        last_date = attr.get(\"last_analysis_date\") or attr.get(\"creation_date\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return stats, rep, last_date\n",
    "\n",
    "# load existing results to support resumable runs\n",
    "if OUT_CSV.exists():\n",
    "    out_df = pd.read_csv(OUT_CSV)\n",
    "    already = set(zip(out_df[\"type\"], out_df[\"value\"]))\n",
    "else:\n",
    "    out_df = pd.DataFrame(columns=[\"type\",\"value\",\"malicious\",\"suspicious\",\"undetected\",\"harmless\",\"reputation\",\"last_analysis_date\",\"http_status\",\"error\"])\n",
    "    already = set()\n",
    "\n",
    "def iter_items():   # ordered priority: file hashes -> urls -> domains -> ips\n",
    "    for kind, seq in ((\"file\", hashes), (\"url\", urls), (\"domain\", domains), (\"ip\", ips)):\n",
    "        for v in seq:\n",
    "            yield kind, v\n",
    "\n",
    "items = list(iter_items())\n",
    "\n",
    "# slice by offset/limit\n",
    "slice_items = items[START_OFFSET: START_OFFSET + MAX_ITEMS]\n",
    "\n",
    "print(f\"Planned lookups this run: {len(slice_items)} (from offset {START_OFFSET})\")\n",
    "\n",
    "rows = []\n",
    "seen = 0\n",
    "last_req_ts = 0.0\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "for kind, value in slice_items:\n",
    "    if VT_ONLY_NEW and (kind, value) in already:    # skip IOCs which are already checked\n",
    "        continue\n",
    "    elapsed = time.time() - last_req_ts\n",
    "    if elapsed < SLEEP_SEC:\n",
    "        time.sleep(SLEEP_SEC - elapsed)\n",
    "    last_req_ts = time.time()\n",
    "\n",
    "    url = vt_url_for(kind, value)\n",
    "    status = None\n",
    "    err = None\n",
    "    stats, rep, when = {}, None, None\n",
    "\n",
    "    try:\n",
    "        r = session.get(url, headers=vt_headers(), timeout=30)\n",
    "        status = r.status_code\n",
    "        if r.status_code == 200:\n",
    "            data = r.json()\n",
    "            stats, rep, when = parse_stats(kind, data)\n",
    "        elif r.status_code in (404, 400):\n",
    "            err = f\"VT status {r.status_code}\"\n",
    "        elif r.status_code == 429:\n",
    "            err = \"Rate limited (429). Increase SLEEP_SEC or lower RATE_LIMIT_QPM.\"\n",
    "        else:\n",
    "            err = f\"HTTP {r.status_code}\"\n",
    "    except Exception as e:\n",
    "        err = str(e)\n",
    "\n",
    "    row = {\n",
    "        \"type\": kind,\n",
    "        \"value\": value,\n",
    "        \"malicious\": stats.get(\"malicious\"),\n",
    "        \"suspicious\": stats.get(\"suspicious\"),\n",
    "        \"undetected\": stats.get(\"undetected\"),\n",
    "        \"harmless\": stats.get(\"harmless\"),\n",
    "        \"reputation\": rep,\n",
    "        \"last_analysis_date\": when,\n",
    "        \"http_status\": status,\n",
    "        \"error\": err\n",
    "    }\n",
    "    rows.append(row)\n",
    "    seen += 1\n",
    "\n",
    "    # periodic checkpoint\n",
    "    if seen % CHUNK_CHECKPOINT == 0:\n",
    "        tmp = pd.DataFrame(rows)\n",
    "        out_df = pd.concat([out_df, tmp], ignore_index=True)\n",
    "        out_df.drop_duplicates(subset=[\"type\",\"value\"], keep=\"last\", inplace=True)\n",
    "        out_df.to_csv(OUT_CSV, index=False)\n",
    "        print(f\"[checkpoint] where {len(out_df)} rows to {OUT_CSV}.\")\n",
    "        rows.clear()\n",
    "\n",
    "# final write\n",
    "if rows:\n",
    "    tmp = pd.DataFrame(rows)\n",
    "    out_df = pd.concat([out_df, tmp], ignore_index=True)\n",
    "    out_df.drop_duplicates(subset=[\"type\",\"value\"], keep=\"last\", inplace=True)\n",
    "    out_df.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(f\"Done. Total rows in {OUT_CSV}: {len(out_df)}.\")\n",
    "\n",
    "# Use filters, filter to only 'malicious >= 1' or negative reputation.\n",
    "flagged = out_df.fillna(0)\n",
    "flagged = flagged[(flagged[\"malicious\"].astype(float) >= 1) | (flagged[\"reputation\"].astype(float) < 0)]\n",
    "flagged_csv = OUT_DIR / \"vt_flagged.csv\"\n",
    "flagged.to_csv(flagged_csv, index=False)\n",
    "print(\"Flagged subset ->\", flagged_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08b82a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 49 IOCs for feeding MISP -> generated\\iocs_misp.csv.\n"
     ]
    }
   ],
   "source": [
    "#5. use filter to keep only \"reputation < 0\" in the final IOC list\n",
    "VT_REPUTATION = OUT_DIR / \"vt_reputation.csv\"\n",
    "IOCS_MISP_CSV = OUT_DIR / \"iocs_misp.csv\"\n",
    "MISP_EVENT_JSON = OUT_DIR / \"misp_event_vtneg.json\"\n",
    "\n",
    "if not VT_REPUTATION.exists():\n",
    "    raise FileNotFoundError(f\"{VT_REPUTATION} not found, run the VT reputation checker first.\")\n",
    "\n",
    "df = pd.read_csv(VT_REPUTATION)\n",
    "# Keep only artifacts that exist in VT that have negative reputation\n",
    "df = df[(df[\"http_status\"] == 200) & (df[\"reputation\"].astype(float) < 0)].copy()\n",
    "\n",
    "# Normalize IOC types for MISP\n",
    "def normalize_type_and_value(row):\n",
    "    t, v = str(row[\"type\"]).lower(), str(row[\"value\"]).strip()\n",
    "    if t == \"ip\":\n",
    "        # allow only valid IPv4 for now\n",
    "        try:\n",
    "            ipaddress.IPv4Address(v)\n",
    "            return \"ipv4\", v\n",
    "        except Exception:\n",
    "            return None, None\n",
    "    elif t == \"domain\":\n",
    "        return \"domain\", v.lower()\n",
    "    elif t == \"url\":\n",
    "        # trim trailing punctuation that might sneak in\n",
    "        v = v.strip(').,;\\'\"')\n",
    "        return \"url\", v\n",
    "    elif t == \"file\":\n",
    "        # decide hash type by length\n",
    "        hv = v.lower()\n",
    "        L = len(hv)\n",
    "        if L == 32 and re.fullmatch(r\"[0-9a-f]{32}\", hv):\n",
    "            return \"md5\", hv\n",
    "        if L == 40 and re.fullmatch(r\"[0-9a-f]{40}\", hv):\n",
    "            return \"sha1\", hv\n",
    "        if L == 64 and re.fullmatch(r\"[0-9a-f]{64}\", hv):\n",
    "            return \"sha256\", hv\n",
    "        return None, None\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "norm = df.apply(lambda r: normalize_type_and_value(r), axis=1, result_type=\"expand\")\n",
    "df[\"norm_type\"], df[\"norm_value\"] = norm[0], norm[1]\n",
    "df = df.dropna(subset=[\"norm_type\",\"norm_value\"]).copy()\n",
    "\n",
    "# Final minimal list for MISP\n",
    "final_iocs = df[[\"norm_type\",\"norm_value\"]].drop_duplicates().rename(\n",
    "    columns={\"norm_type\":\"type\", \"norm_value\":\"value\"}\n",
    ")\n",
    "final_iocs.to_csv(IOCS_MISP_CSV, index=False)\n",
    "print(f\"Prepared {len(final_iocs)} IOCs for feeding MISP -> {IOCS_MISP_CSV}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a40cc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM available: True\n",
      "Wrote: generated\\iocs_enriched.json\n",
      "Example: {\n",
      "  \"type\": \"url\",\n",
      "  \"value\": \"https://api.w.org/\",\n",
      "  \"context\": \"This IOC likely represents a benign URL associated with a web API, potentially misused in a phishing or C2 context.\",\n",
      "  \"attack_mapping\": [],\n",
      "  \"confidence\": 30,\n",
      "  \"rationale\": \"The VirusTotal analysis shows a high number of harmless detections and no malicious flags, indicating that the URL is likely not directly associated with malicious activity, but its usage context could be suspicious.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#6. LLM enrichment for the final IOCs\n",
    "#1) load the data already ingested before\n",
    "if 'ioc_df' not in globals() or ioc_df.empty:\n",
    "    raise RuntimeError(\"ioc_df is missing/empty. Run IOC extraction first.\")\n",
    "\n",
    "def safe_ready(p: Path):\n",
    "    try:\n",
    "        return p.read_text(errors=\"ignore\")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "texts = {}\n",
    "if DATA_DIR.exists():\n",
    "    for p in DATA_DIR.glob(\"**/*\"):\n",
    "        if p.is_file() and p.suffix.lower() in {\".txt\",\".md\",\".html\",\"htm\",\".log\"}:\n",
    "            texts[p.name] = safe_ready(p)\n",
    "\n",
    "#2) pull VT verification results to add signals to the prompt\n",
    "VT_PATH = OUT_DIR / \"vt_reputation.csv\"\n",
    "vt_df = pd.read_csv(VT_PATH) if VT_PATH.exists() else pd.DataFrame(columns=[\"type\",\"value\"])\n",
    "def vt_row_for(ioc_type, ioc_value):\n",
    "    if vt_df.empty:\n",
    "        return None\n",
    "    m = vt_df[(vt_df[\"type\"].astype(str)==ioc_type) & (vt_df[\"value\"].astype(str)==ioc_value)]\n",
    "    return m.iloc[0].to_dict() if not m.empty else None\n",
    "\n",
    "#3) build context snippets around each IOC value from its source text\n",
    "def gather_snippets(row, window=160):\n",
    "    src = str(row.get(\"source\", \"\"))\n",
    "    val = str(row.get(\"value\", \"\"))\n",
    "    text = texts.get(src, \"\")\n",
    "    if not text or not val:\n",
    "        return []\n",
    "    pattern = re.escape(val)\n",
    "    snippets = []\n",
    "    for m in re.finditer(pattern, text, flags=re.IGNORECASE):\n",
    "        start = max(0, m.start() - window)\n",
    "        end = min(len(text), m.end() + window)\n",
    "        snippets.append(text[start:end].replace(\"\\n\", \" \"))\n",
    "        if len(snippets) >= 3:\n",
    "            break\n",
    "    return snippets\n",
    "\n",
    "#4) LLM call per IOC\n",
    "USE_LLM = bool(OPENAI_API_KEY)\n",
    "print(\"LLM available:\", USE_LLM)\n",
    "\n",
    "def build_prompt(ioc_item, snippets, vt):\n",
    "    evidence_lines = []\n",
    "    if vt:\n",
    "        ev = {\n",
    "            \"vt_http_status\": vt.get(\"http_status\"),\n",
    "            \"vt_reputation\": vt.get(\"reputation\"),\n",
    "            \"vt_malicious\": vt.get(\"malicious\"),\n",
    "            \"vt_suspicious\": vt.get(\"suspicious\"),\n",
    "            \"vt_undetected\": vt.get(\"undetected\"),\n",
    "            \"vt_harmless\": vt.get(\"harmless\"),\n",
    "        }\n",
    "        evidence_lines.append(f\"VirusTotal: {ev}\")\n",
    "    if snippets:\n",
    "        evidence_lines.append(f\"Local context: {snippets[:2]}\")\n",
    "    return f\"\"\"\n",
    "You are a CTI analyst. Given a single IOC and evidence, infer likely context and ATT&CK techniques.\n",
    "Return STRICT JSON with keys:\n",
    "- \"context\": short one-sentence summary of what this IOC likely represents (C2, payload, phish link, scanner, etc.)\n",
    "- \"attack_mapping\": array of ATT&CK technique IDs (e.g., [\"T1071\",\"T1105\"]); include only techniques you can justify\n",
    "- \"confidence\": integer 0-100 for your overall assessment\n",
    "- \"rationale\": 1-2 sentence justification referencing the evidence\n",
    "\n",
    "IOC:\n",
    "  type: {ioc_item.get('type')}\n",
    "  value: {ioc_item.get('value')}\n",
    "Evidence:\n",
    "  {os.linesep.join(evidence_lines) if evidence_lines else \"No external evidence.\"}\n",
    "\n",
    "Constraints:\n",
    "- Base your mapping on the evidence only. If insufficient, return an empty array for \"attack_mapping\" and low confidence.\n",
    "- Use only valid ATT&CK technique IDs (Txxxx).\n",
    "- JSON only, no extra keys or text.\n",
    "\"\"\".strip()\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY) # pass api key to openai client\n",
    "\n",
    "def call_llm_json(prompt, model=\"gpt-4o-mini\", temperature=0.2):\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            response_format={\"type\":\"json_object\"},\n",
    "            messages=[\n",
    "                {\"role\":\"system\",\"content\":\"You output strict JSON only.\"},\n",
    "                {\"role\":\"user\",\"content\":prompt}\n",
    "            ],\n",
    "        )\n",
    "        return json.loads(resp.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        return {\"context\":\"Insufficient enrichment (LLM error).\",\"attack_mapping\":[],\"confidence\":5,\"rationale\":str(e)[:200]}\n",
    "\n",
    "#5) run enrichment per IOC\n",
    "records = []\n",
    "if OPENAI_API_KEY:\n",
    "    QPS = float(os.getenv(\"LLM_QPS\", \"2\"))\n",
    "    sleep_s = max(0.0, 1.0 / max(0.1, QPS))\n",
    "    for _, item in ioc_df.iterrows():\n",
    "        snippets = gather_snippets(item)\n",
    "        vt = vt_row_for(item[\"type\"], item[\"value\"])\n",
    "        prompt = build_prompt(item, snippets, vt)\n",
    "        enriched = call_llm_json(prompt)\n",
    "        records.append({**item.to_dict(), **enriched})\n",
    "        time.sleep(sleep_s)\n",
    "else:\n",
    "    for _, item in ioc_df.iterrows():\n",
    "        records.append({\n",
    "            **item.to_dict(),\n",
    "            \"context\":\"(no LLM - pass-through)\",\n",
    "            \"attack_mapping\":[],\n",
    "            \"confidence\":0,\n",
    "            \"rationale\":\"LLM enrichment disabled.\"\n",
    "        })\n",
    "#6) save enriched output\n",
    "enriched_json = OUT_DIR / \"iocs_enriched.json\"\n",
    "with open(enriched_json, \"w\") as file:\n",
    "    json.dump(records, file, indent=2)\n",
    "print(\"Wrote:\", enriched_json)\n",
    "print(\"Example:\", json.dumps(records[48], indent=2)[:600] if records else \"(no records)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9fd5d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded final_iocs: 49 rows\n",
      "Prepared for MISP (by type):\n",
      "type\n",
      "md5        1\n",
      "sha1      34\n",
      "sha256    11\n",
      "url        3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronal\\AppData\\Local\\Temp\\ipykernel_17868\\1537916341.py:76: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"date\": datetime.utcnow().strftime(\"%Y-%m-%d\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created event id: 10493 → generated\\misp_event_from_final_iocs.json\n",
      "Attributes added: 49 / 49\n"
     ]
    }
   ],
   "source": [
    "#7. feed final iocs to MISP\n",
    "OUT_DIR = Path(\"./generated\"); OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "FINAL_IOCS = OUT_DIR / \"iocs_misp.csv\"               # produced by VT 200-only gate\n",
    "MISP_EVENT_JSON = OUT_DIR / \"misp_event_from_final_iocs.json\"\n",
    "SKIPPED_CSV = OUT_DIR / \"misp_skipped.csv\"\n",
    "FAILED_CSV  = OUT_DIR / \"misp_failed.csv\"\n",
    "\n",
    "if not FINAL_IOCS.exists():\n",
    "    raise FileNotFoundError(f\"{FINAL_IOCS} not found. Run the VT 200-only gate cell first.\")\n",
    "\n",
    "df = pd.read_csv(FINAL_IOCS).dropna().drop_duplicates()\n",
    "print(f\"Loaded final_iocs: {len(df)} rows\")\n",
    "\n",
    "def normalize_type_and_value(t, v):\n",
    "    t = str(t).lower().strip()\n",
    "    v = str(v).strip()\n",
    "\n",
    "    # URLs\n",
    "    if t == \"url\":\n",
    "        return {\"type\": \"url\", \"value\": v.strip(').,;\\'\"'), \"category\": \"Network activity\"}\n",
    "\n",
    "    # Domains\n",
    "    if t == \"domain\":\n",
    "        return {\"type\": \"domain\", \"value\": v.lower(), \"category\": \"Network activity\"}\n",
    "\n",
    "    # IPs\n",
    "    if t in (\"ip\",\"ipv4\",\"ip-dst\",\"ip-src\"):\n",
    "        try:\n",
    "            ipaddress.IPv4Address(v)\n",
    "            # Default to destination IP observable for blocklists; adjust if you track src instead\n",
    "            return {\"type\": \"ip-dst\", \"value\": v, \"category\": \"Network activity\"}\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    # File hashes\n",
    "    if t in (\"file\",\"md5\",\"sha1\",\"sha256\"):\n",
    "        hv = v.lower()\n",
    "        if t == \"md5\" or (len(hv) == 32 and re.fullmatch(r\"[0-9a-f]{32}\", hv)):\n",
    "            return {\"type\": \"md5\", \"value\": hv, \"category\": \"Artifacts dropped\"}\n",
    "        if t == \"sha1\" or (len(hv) == 40 and re.fullmatch(r\"[0-9a-f]{40}\", hv)):\n",
    "            return {\"type\": \"sha1\", \"value\": hv, \"category\": \"Artifacts dropped\"}\n",
    "        if t == \"sha256\" or (len(hv) == 64 and re.fullmatch(r\"[0-9a-f]{64}\", hv)):\n",
    "            return {\"type\": \"sha256\", \"value\": hv, \"category\": \"Artifacts dropped\"}\n",
    "        return None\n",
    "\n",
    "    # skip anything else (e.g., email) for this push\n",
    "    return None\n",
    "\n",
    "norm_rows, skipped = [], []\n",
    "for _, r in df.iterrows():\n",
    "    norm = normalize_type_and_value(r[\"type\"], r[\"value\"])\n",
    "    if norm:\n",
    "        norm_rows.append(norm)\n",
    "    else:\n",
    "        skipped.append({\"orig_type\": r[\"type\"], \"orig_value\": r[\"value\"], \"reason\": \"failed normalization\"})\n",
    "\n",
    "norm_df = pd.DataFrame(norm_rows).drop_duplicates()\n",
    "if skipped:\n",
    "    pd.DataFrame(skipped).to_csv(SKIPPED_CSV, index=False)\n",
    "    print(f\"Skipped during normalization: {len(skipped)} → {SKIPPED_CSV}\")\n",
    "\n",
    "print(\"Prepared for MISP (by type):\")\n",
    "print(norm_df.groupby(\"type\").size().to_string())\n",
    "\n",
    "# Build event (no attributes yet)\n",
    "if not (MISP_URL and MISP_KEY):\n",
    "    raise RuntimeError(\"Set MISP_URL and MISP_KEY env vars to push to MISP.\")\n",
    "\n",
    "session = requests.Session()\n",
    "headers = {\"Authorization\": MISP_KEY, \"Accept\": \"application/json\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "event_body = {\n",
    "    \"Event\": {\n",
    "        \"info\": \"Home-lab: final_iocs (VT 200-only) pushed to MISP\",\n",
    "        \"analysis\": 2,             # 0=initial,1=ongoing,2=completed\n",
    "        \"threat_level_id\": 2,      # 1=high,2=medium,3=low,4=undefined\n",
    "        \"date\": datetime.utcnow().strftime(\"%Y-%m-%d\"),\n",
    "        \"distribution\": 0          # 0=Your org only (safe default)\n",
    "    }\n",
    "}\n",
    "\n",
    "resp = session.post(f\"{MISP_URL}/events/add\", headers=headers, json=event_body, timeout=30, verify=False)\n",
    "if resp.status_code != 200:\n",
    "    raise RuntimeError(f\"Event creation failed: {resp.status_code} {resp.text}\")\n",
    "\n",
    "event = resp.json().get(\"Event\") or resp.json().get(\"event\") or {}\n",
    "event_id = event.get(\"id\")\n",
    "if not event_id:\n",
    "    raise RuntimeError(f\"Could not obtain event_id from response: {resp.text}\")\n",
    "\n",
    "with open(MISP_EVENT_JSON, \"w\") as f:\n",
    "    json.dump(resp.json(), f, indent=2)\n",
    "print(\"Created event id:\", event_id, \"→\", MISP_EVENT_JSON)\n",
    "\n",
    "# Add attributes one-by-one so we can see which ones fail\n",
    "failed = []\n",
    "added = 0\n",
    "for _, row in norm_df.iterrows():\n",
    "    attr = {\n",
    "        \"type\": row[\"type\"],\n",
    "        \"category\": row[\"category\"],\n",
    "        \"value\": row[\"value\"],\n",
    "        \"to_ids\": True,\n",
    "        \"distribution\": 0\n",
    "    }\n",
    "    r = session.post(f\"{MISP_URL}/attributes/add/{event_id}\", headers=headers, json={\"Attribute\": attr}, timeout=30, verify=False)\n",
    "    if r.status_code == 200:\n",
    "        added += 1\n",
    "    else:\n",
    "        failed.append({\n",
    "            \"type\": row[\"type\"],\n",
    "            \"value\": row[\"value\"],\n",
    "            \"status\": r.status_code,\n",
    "            \"body\": r.text[:300]\n",
    "        })\n",
    "\n",
    "print(f\"Attributes added: {added} / {len(norm_df)}\")\n",
    "if failed:\n",
    "    pd.DataFrame(failed).to_csv(FAILED_CSV, index=False)\n",
    "    print(f\"Some attributes failed to add: {len(failed)} → {FAILED_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c2c5f2",
   "metadata": {},
   "source": [
    "## Part 2 - Detection Rule Generation & Traffic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c300760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 3 Suricata rules -> generated\\llm_generated.rules.\n"
     ]
    }
   ],
   "source": [
    "#1. generate Suricata detection rules\n",
    "from ipaddress import ip_address        # validate an ipv4 value is a real IP\n",
    "\n",
    "RULES_PATH = OUT_DIR / \"llm_generated.rules\"\n",
    "\n",
    "SURICATA_IP_RULE = \"alert ip any any -> {target} any (msg:\\\"IOC match: {value}\\\"; sid:{sid}; rev:1;)\"\n",
    "SURICATA_DOMAIN_RULE = \"alert dns any any -> any any (msg:\\\"IOC domain: {value}\\\"; dns.query; content:\\\"{value}\\\"; sid:{sid}; rev:1;)\"\n",
    "SURICATA_URL_RULE = \"alert http any any -> any any (msg:\\\"IOC URL: {value}\\\"; flow:to_server,established; http.host; content:\\\"{host}\\\"; http.uri; content:\\\"{path}\\\"; sid:{sid}; rev:1;)\"\n",
    "\n",
    "sid_base = 4200000      # seeds rule IDs and increments for each emitted rule\n",
    "rules = []\n",
    "seen = set()            # prevents duplicate (type, value) rules\n",
    "\n",
    "for _, row in ioc_df.iterrows():\n",
    "    t, v = row['type'], row['value']\n",
    "    if (t, v) in seen:\n",
    "        continue\n",
    "    seen.add((t, v))\n",
    "    sid_base += 1\n",
    "    if t == 'ipv4':\n",
    "        try:\n",
    "            ip_address(v)\n",
    "            rules.append(SURICATA_IP_RULE.format(target=v, value=v, sid=sid_base))\n",
    "        except Exception:\n",
    "            pass\n",
    "    elif t == 'domain':\n",
    "        rules.append(SURICATA_DOMAIN_RULE.format(value=v, sid=sid_base))\n",
    "    elif t == 'url':\n",
    "        m = re.match(f\"https?://([^/]+)(/.*)?\", v)\n",
    "        if m:\n",
    "            host = m.group(1)\n",
    "            path = m.group(2) or \"/\"\n",
    "            rules.append(SURICATA_URL_RULE.format(value=v, host=host, path=path, sid=sid_base))\n",
    "\n",
    "with open(RULES_PATH, \"w\") as file:\n",
    "    for r in rules:\n",
    "        file.write(r + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {len(rules)} Suricata rules -> {RULES_PATH}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1331e8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 EVE files.\n",
      "Flows: (3, 5) Alerts: (3, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_ip</th>\n",
       "      <th>dest_ip</th>\n",
       "      <th>proto</th>\n",
       "      <th>bytes_toserver</th>\n",
       "      <th>bytes_toclient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0.0.5</td>\n",
       "      <td>185.199.110.153</td>\n",
       "      <td>TCP</td>\n",
       "      <td>2048</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0.0.6</td>\n",
       "      <td>185.199.110.153</td>\n",
       "      <td>TCP</td>\n",
       "      <td>1024</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0.0.7</td>\n",
       "      <td>185.199.110.153</td>\n",
       "      <td>TCP</td>\n",
       "      <td>2048</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     src_ip          dest_ip proto  bytes_toserver  bytes_toclient\n",
       "0  10.0.0.5  185.199.110.153   TCP            2048             512\n",
       "1  10.0.0.6  185.199.110.153   TCP            1024             512\n",
       "2  10.0.0.7  185.199.110.153   TCP            2048            1024"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. parse Suricata EVE logs and run anomaly detection\n",
    "def iter_eve_records(paths):\n",
    "    for p in paths:\n",
    "        if p.suffix == \".gz\":\n",
    "            with gzip.open(p, \"rt\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "                for line in fh:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    try:\n",
    "                        yield json_fast.loads(line)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "        else:\n",
    "            with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "                for line in fh:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    try:\n",
    "                        yield json_fast.loads(line)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "eve_paths = list(Path(\"./eve_inputs\").glob(\"*.json\"))\n",
    "print(f\"Found {len(eve_paths)} EVE files.\")\n",
    "\n",
    "flows = []\n",
    "alerts = []\n",
    "for rec in iter_eve_records(eve_paths):\n",
    "    et = rec.get(\"event_type\")\n",
    "    if et == \"flow\":\n",
    "        flows.append({\n",
    "            \"src_ip\": rec.get(\"src_ip\"),\n",
    "            \"dest_ip\": rec.get(\"dest_ip\"),\n",
    "            \"proto\": rec.get(\"proto\"),\n",
    "            \"bytes_toserver\": rec.get(\"bytes_toserver\", 0),\n",
    "            \"bytes_toclient\": rec.get(\"bytes_toclient\", 0)\n",
    "        })\n",
    "    elif et == \"alert\":\n",
    "        a = rec.get(\"alert\", {})\n",
    "        alerts.append({\n",
    "            \"src_ip\": rec.get(\"src_ip\"),\n",
    "            \"dest_ip\": rec.get(\"dest_ip\"),\n",
    "            \"signature_id\": a.get(\"signature_id\"),\n",
    "            \"signature\": a.get(\"signature\")\n",
    "        })\n",
    "\n",
    "flows_df = pd.DataFrame(flows)\n",
    "alerts_df = pd.DataFrame(alerts)\n",
    "print(\"Flows:\", flows_df.shape, \"Alerts:\", alerts_df.shape)\n",
    "flows_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441161ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly summary -> generated\\anomaly_hosts.csv\n"
     ]
    }
   ],
   "source": [
    "#3. aggregate features per src_ip for anomaly detection\n",
    "if not flows_df.empty:\n",
    "    agg = flows_df.groupby(\"src_ip\").agg(\n",
    "        flows=(\"proto\",\"count\"),\n",
    "        bytes_up=(\"bytes_toserver\",\"sum\"),\n",
    "        bytes_down=(\"bytes_toclient\",\"sum\")\n",
    "    ).reset_index()\n",
    "\n",
    "    model = IsolationForest(n_estimators=150, contamination=0.05, random_state=42)\n",
    "    X = agg[[\"flows\",\"bytes_up\",\"bytes_down\"]]\n",
    "    model.fit(X)\n",
    "    agg[\"anomaly_score\"] = model.decision_function(X)\n",
    "    agg[\"is_anomaly\"] = model.predict(X) == -1\n",
    "\n",
    "    agg_csv = OUT_DIR / \"anomaly_hosts.csv\"\n",
    "    agg.to_csv(agg_csv, index=False)\n",
    "    print(\"Anomaly summary ->\", agg_csv)\n",
    "\n",
    "    display_cols = [\"src_ip\",\"flows\",\"bytes_up\",\"bytes_down\",\"anomaly_score\",\"is_anomaly\"]\n",
    "    agg.sort_values(\"anomaly_score\").head(10)[display_cols]\n",
    "else:\n",
    "    print(\"No flows found; add EVE flow records to run anomaly detection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb00db17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1 anomaly-based rules -> generated\\anomaly_proposed.rules.\n"
     ]
    }
   ],
   "source": [
    "#4. draft rules from anomalies & correlate with IOCs\n",
    "proposed_rules = []\n",
    "if 'agg' in globals() and not agg.empty:\n",
    "    suspicious = agg[agg['is_anomaly']]\n",
    "    suspicious_ips = set(suspicious['src_ip'].dropna().tolist())\n",
    "    ioc_ips = set(ioc_df.loc[ioc_df['type']==\"ipv4\", 'value'].tolist())\n",
    "\n",
    "    sid = 4300000\n",
    "    for ip in suspicious_ips:\n",
    "        sid += 1\n",
    "        msg = \"Anomalous host detected\"\n",
    "        if ip in ioc_ips:\n",
    "            msg = \"Anomalous host matched IOC\"\n",
    "        proposed_rules.append(f\"alert ip {ip} any -> any any (msg:\\\"{msg}\\\"; sid:{sid}; rev:1;)\")\n",
    "\n",
    "    prop_path = OUT_DIR / \"anomaly_proposed.rules\"\n",
    "    with open(prop_path, \"w\") as file:\n",
    "        for r in proposed_rules:\n",
    "            file.write(r + \"\\n\")\n",
    "    print(f\"Wrote {len(proposed_rules)} anomaly-based rules -> {prop_path}.\")\n",
    "else:\n",
    "    print(\"No anomalies to propose rules from.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bbc299e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs in: C:\\Users\\ronal\\Desktop\\TDLab\\generated\n",
      "- anomaly_hosts.csv\n",
      "- anomaly_proposed.rules\n",
      "- iocs_enriched.json\n",
      "- iocs_extracted.csv\n",
      "- iocs_for_vt.csv\n",
      "- iocs_misp.csv\n",
      "- llm_generated.rules\n",
      "- misp_event.json\n",
      "- misp_event_from_final_iocs.json\n",
      "- misp_event_vtneg.json\n",
      "- vt_flagged.csv\n",
      "- vt_reputation.csv\n"
     ]
    }
   ],
   "source": [
    "# final outputs\n",
    "print(\"Outputs in:\", OUT_DIR.resolve())\n",
    "for p in sorted(OUT_DIR.glob(\"*\")):\n",
    "    print(\"-\", p.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9b210720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next steps:\n",
      "1) Load the generated .rules into Suricata and test against pcap files.\n",
      "2) Hook the notebook into a scheduler or convert to scripts for cron jobs.\n",
      "3) Add LLM calls for richer enrichment & rule commentary.\n"
     ]
    }
   ],
   "source": [
    "# next step\n",
    "print(\"Next steps:\")\n",
    "print(\"1) Load the generated .rules into Suricata and test against pcap files.\")\n",
    "print(\"2) Hook the notebook into a scheduler or convert to scripts for cron jobs.\")\n",
    "print(\"3) Add LLM calls for richer enrichment & rule commentary.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-08-10T03:40:58.690535Z",
     "iopub.status.busy": "2025-08-10T03:40:58.690252Z",
     "iopub.status.idle": "2025-08-10T03:41:03.933797Z",
     "shell.execute_reply": "2025-08-10T03:41:03.933111Z",
     "shell.execute_reply.started": "2025-08-10T03:40:58.690511Z"
    },
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1754684818830,
     "user": {
      "displayName": "Jack Si",
      "userId": "03164754694381449649"
     },
     "user_tz": 360
    },
    "id": "SouS-Ac9_wHk",
    "outputId": "9c992094-87e4-4495-b9a5-eee2bb7c4539",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"GPU available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-08-10T03:41:08.540401Z",
     "iopub.status.busy": "2025-08-10T03:41:08.539368Z",
     "iopub.status.idle": "2025-08-10T03:42:36.108804Z",
     "shell.execute_reply": "2025-08-10T03:42:36.107906Z",
     "shell.execute_reply.started": "2025-08-10T03:41:08.540374Z"
    },
    "executionInfo": {
     "elapsed": 131336,
     "status": "ok",
     "timestamp": 1754683907484,
     "user": {
      "displayName": "Jack Si",
      "userId": "03164754694381449649"
     },
     "user_tz": 360
    },
    "id": "JM9--qLm_yAI",
    "outputId": "1531ff6f-9cee-42ad-982a-20e1afbf9026",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#1. install required packages\n",
    "!pip install -q bitsandbytes transformers accelerate peft datasets sentencepiece safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "execution": {
     "iopub.execute_input": "2025-08-10T03:42:40.253683Z",
     "iopub.status.busy": "2025-08-10T03:42:40.252816Z",
     "iopub.status.idle": "2025-08-10T03:42:40.258960Z",
     "shell.execute_reply": "2025-08-10T03:42:40.258104Z",
     "shell.execute_reply.started": "2025-08-10T03:42:40.253639Z"
    },
    "executionInfo": {
     "elapsed": 4326,
     "status": "error",
     "timestamp": 1754683911813,
     "user": {
      "displayName": "Jack Si",
      "userId": "03164754694381449649"
     },
     "user_tz": 360
    },
    "id": "w5i19walABf3",
    "outputId": "d50dff9f-4f77-4f5f-bd37-ee9d47069810",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving outputs to /kaggle/working/ti_llm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "DRIVE_BASE = '/kaggle/working/ti_llm'\n",
    "os.makedirs(DRIVE_BASE, exist_ok=True)\n",
    "print('Saving outputs to', DRIVE_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T03:42:44.513642Z",
     "iopub.status.busy": "2025-08-10T03:42:44.513077Z",
     "iopub.status.idle": "2025-08-10T03:42:45.110617Z",
     "shell.execute_reply": "2025-08-10T03:42:45.110079Z",
     "shell.execute_reply.started": "2025-08-10T03:42:44.513616Z"
    },
    "executionInfo": {
     "elapsed": 135857,
     "status": "aborted",
     "timestamp": 1754683911803,
     "user": {
      "displayName": "Jack Si",
      "userId": "03164754694381449649"
     },
     "user_tz": 360
    },
    "id": "AsjbGYVRDjw9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#3. Hugging Face login\n",
    "HF_TOKEN = 'REPLACE_WITH_YOUR_HF_TOKEN'  # Hugging Face token\n",
    "if HF_TOKEN:\n",
    "    from huggingface_hub import login\n",
    "    login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T03:42:50.574225Z",
     "iopub.status.busy": "2025-08-10T03:42:50.573927Z",
     "iopub.status.idle": "2025-08-10T03:42:50.577952Z",
     "shell.execute_reply": "2025-08-10T03:42:50.577106Z",
     "shell.execute_reply.started": "2025-08-10T03:42:50.574205Z"
    },
    "executionInfo": {
     "elapsed": 135859,
     "status": "aborted",
     "timestamp": 1754683911806,
     "user": {
      "displayName": "Jack Si",
      "userId": "03164754694381449649"
     },
     "user_tz": 360
    },
    "id": "HqYP_hSEDX_6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#4. choose base model\n",
    "MODEL_NAME = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "MAX_LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T03:42:56.269363Z",
     "iopub.status.busy": "2025-08-10T03:42:56.269118Z",
     "iopub.status.idle": "2025-08-10T03:42:57.147799Z",
     "shell.execute_reply": "2025-08-10T03:42:57.147022Z",
     "shell.execute_reply.started": "2025-08-10T03:42:56.269344Z"
    },
    "executionInfo": {
     "elapsed": 135851,
     "status": "aborted",
     "timestamp": 1754683911807,
     "user": {
      "displayName": "Jack Si",
      "userId": "03164754694381449649"
     },
     "user_tz": 360
    },
    "id": "uJs5MfQpD-c0",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISP actors fetched: 857\n",
      "MITRE ATT&CK techniques fetched: 199\n"
     ]
    }
   ],
   "source": [
    "#5. download a small threat intelligence dataset\n",
    "import requests, json\n",
    "\n",
    "misp_url = 'https://raw.githubusercontent.com/MISP/misp-galaxy/main/clusters/threat-actor.json'\n",
    "resp = requests.get(misp_url)\n",
    "misp = resp.json()\n",
    "values = misp.get('values', [])\n",
    "print('MISP actors fetched:', len(values))\n",
    "\n",
    "attack_url = 'https://raw.githubusercontent.com/mitre-attack/attack-stix-data/master/enterprise-attack/enterprise-attack-17.0.json'\n",
    "resp2 = requests.get(attack_url)\n",
    "attack = resp2.json()\n",
    "techniques = []\n",
    "for obj in attack.get('objects', [])[:200]:\n",
    "  if obj.get('type') == 'attack-pattern':\n",
    "    name = obj.get('name')\n",
    "    desc = obj.get('description', '')\n",
    "    techniques.append({'name': name, 'description': desc})\n",
    "print('MITRE ATT&CK techniques fetched:', len(techniques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T03:43:01.372016Z",
     "iopub.status.busy": "2025-08-10T03:43:01.371303Z",
     "iopub.status.idle": "2025-08-10T03:43:01.382651Z",
     "shell.execute_reply": "2025-08-10T03:43:01.381914Z",
     "shell.execute_reply.started": "2025-08-10T03:43:01.371987Z"
    },
    "executionInfo": {
     "elapsed": 135848,
     "status": "aborted",
     "timestamp": 1754683911809,
     "user": {
      "displayName": "Jack Si",
      "userId": "03164754694381449649"
     },
     "user_tz": 360
    },
    "id": "wkxwTKv8Gug8",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training pairs: 200\n"
     ]
    }
   ],
   "source": [
    "#6. build small instruction-response pairs training dataset\n",
    "train_pairs = []\n",
    "for actor in values[:80]:   # limit MISP actors to 80 for test\n",
    "  val = actor.get('value')\n",
    "  desc = actor.get('description') or actor.get('meta', {}).get('description', 'No description available')\n",
    "  instr = f\"Tell me abou the threat actor {val}.\"\n",
    "  resp = desc\n",
    "  train_pairs.append({'instruction': instr, 'output': resp})\n",
    "\n",
    "for tech in techniques[:120]:   # limit MITRE ATT&CK techniques to 120 for test\n",
    "  instr = f\"Explain the ATT&CK technique {tech['name']}.\"\n",
    "  resp = tech['description'] or 'Description not available.'\n",
    "  train_pairs.append({'instruction': instr, 'output': resp})\n",
    "\n",
    "print('Total training pairs:', len(train_pairs))\n",
    "\n",
    "with open(os.path.join(DRIVE_BASE, 'ti_train_pairs.json'), 'w') as file:\n",
    "  json.dump(train_pairs, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T03:43:06.080367Z",
     "iopub.status.busy": "2025-08-10T03:43:06.079853Z",
     "iopub.status.idle": "2025-08-10T03:43:07.531234Z",
     "shell.execute_reply": "2025-08-10T03:43:07.530403Z",
     "shell.execute_reply.started": "2025-08-10T03:43:06.080335Z"
    },
    "executionInfo": {
     "elapsed": 135843,
     "status": "aborted",
     "timestamp": 1754683911810,
     "user": {
      "displayName": "Jack Si",
      "userId": "03164754694381449649"
     },
     "user_tz": 360
    },
    "id": "EWemuNDuINge",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c232f85be043de814669e7940fbe07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Tell me abou the threat actor APT1.', 'output': \"PLA Unit 61398 (Chinese: 61398部队, Pinyin: 61398 bùduì) is the Military Unit Cover Designator (MUCD)[1] of a People's Liberation Army advanced persistent threat unit that has been alleged to be a source of Chinese computer hacking attacks\", 'text': \"### Instruction:\\nTell me abou the threat actor APT1.\\n\\n### Response:\\nPLA Unit 61398 (Chinese: 61398部队, Pinyin: 61398 bùduì) is the Military Unit Cover Designator (MUCD)[1] of a People's Liberation Army advanced persistent threat unit that has been alleged to be a source of Chinese computer hacking attacks\"}\n"
     ]
    }
   ],
   "source": [
    "#7. prepare dataset with 'datasets' library\n",
    "from datasets import Dataset\n",
    "\n",
    "def to_prompt(x):\n",
    "  # Use .get() to safely access keys and provide a default value\n",
    "  instruction = x.get('instruction', '')\n",
    "  output = x.get('output', '')\n",
    "\n",
    "  prompt = (\n",
    "      \"### Instruction:\\n\" + instruction + \"\\n\\n\"\n",
    "      + \"### Response:\\n\" + output\n",
    "  )\n",
    "  return {'text': prompt}\n",
    "\n",
    "raw_ds = Dataset.from_list(train_pairs)\n",
    "ds = raw_ds.map(to_prompt)\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T03:43:14.515928Z",
     "iopub.status.busy": "2025-08-10T03:43:14.515246Z",
     "iopub.status.idle": "2025-08-10T03:43:20.998454Z",
     "shell.execute_reply": "2025-08-10T03:43:20.997793Z",
     "shell.execute_reply.started": "2025-08-10T03:43:14.515894Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1754683911868,
     "user": {
      "displayName": "Jack Si",
      "userId": "03164754694381449649"
     },
     "user_tz": 360
    },
    "id": "rJ_wQ8G9Jg1w",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e7215e2f964fd0a4e4cf66dbdc8e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8760410a46f940ebb33b68c92c7f13af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e708cf9cf9e14c85a79ecdd562de2888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7aca0f21d7347ff97dd9b452eeb99fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f41c6c11d8429280b52f8dfa37be55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Tell me abou the threat actor APT1.', 'output': \"PLA Unit 61398 (Chinese: 61398部队, Pinyin: 61398 bùduì) is the Military Unit Cover Designator (MUCD)[1] of a People's Liberation Army advanced persistent threat unit that has been alleged to be a source of Chinese computer hacking attacks\", 'input_ids': [1, 27332, 3133, 3112, 28747, 13, 22467, 528, 534, 280, 272, 5483, 10964, 330, 6316, 28740, 28723, 13, 13, 27332, 12107, 28747, 13, 3898, 28741, 13332, 28705, 28784, 28740, 28770, 28774, 28783, 325, 1209, 5965, 28747, 28705, 28784, 28740, 28770, 28774, 28783, 29237, 30031, 28725, 367, 4279, 262, 28747, 28705, 28784, 28740, 28770, 28774, 28783, 287, 28912, 670, 28943, 28731, 349, 272, 19844, 13332, 16881, 8648, 1028, 325, 28755, 28779, 5072, 10908, 28740, 28793, 302, 264, 5619, 28742, 28713, 8592, 352, 8094, 10023, 24777, 5483, 5028, 369, 659, 750, 22362, 298, 347, 264, 2832, 302, 6707, 6074, 14413, 288, 10813, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "#8. tokenize dataset and create datacollator\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "  return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=MAX_LENGTH)\n",
    "\n",
    "tokenized_ds = ds.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "print(tokenized_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T03:43:27.441235Z",
     "iopub.status.busy": "2025-08-10T03:43:27.440732Z",
     "iopub.status.idle": "2025-08-10T03:46:32.098342Z",
     "shell.execute_reply": "2025-08-10T03:46:32.097437Z",
     "shell.execute_reply.started": "2025-08-10T03:43:27.441213Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1754683911871,
     "user": {
      "displayName": "Jack Si",
      "userId": "03164754694381449649"
     },
     "user_tz": 360
    },
    "id": "cThqWK58LJIG",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-10 03:43:34.004409: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754797414.274224      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754797414.346227      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 4-bit quantized model (this may take a few minutes)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e7f8d22312447fb037d402a2a9e7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf6b9dfa4ce4a7daa449026a2e6fa52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3bc9395d15745aab9ba467967373b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8431a310bf449187dbcbc8e78c430d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c525798cf8462a9c3862e10b6848da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fafb179f9bd45ed955cd1d92f989ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6785a033d54d6c96e460dad62af8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c006dd0694b04297a555c2db45c5b614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model wrapped with LoRA.\n"
     ]
    }
   ],
   "source": [
    "#9. QLoRA + LoRA setup\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print('Loading 4-bit quantized model (this may take a few minutes)...')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.to('cuda:0')\n",
    "\n",
    "# config LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print('Model wrapped with LoRA.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T03:33:01.271256Z",
     "iopub.status.busy": "2025-08-10T03:33:01.270473Z",
     "iopub.status.idle": "2025-08-10T03:33:21.737496Z",
     "shell.execute_reply": "2025-08-10T03:33:21.736614Z",
     "shell.execute_reply.started": "2025-08-10T03:33:01.271220Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Base model ===\n",
      "### Instruction:\n",
      "Explain the ATT&CK technique Rc.common.\n",
      "\n",
      "### Response:\n",
      "\n",
      "The Rc.common technique, as defined in the MITRE ATT&CK framework, refers to the use of Remote Command and Shell (Rc) tools to execute commands on a target system. These tools can be used for various malicious purposes, such as data exfiltration, privilege escalation, or installing additional malware.\n",
      "\n",
      "Rc.common is a tactic within the Remote Access (R) category of the ATT&CK framework. It specifically describes the use of remote command and shell tools to gain access to a target system and execute commands. This can be achieved through various means, such as exploiting vulnerabilities in software, using stolen credentials, or exploiting misconfigured services.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test base model before training\n",
    "import torch\n",
    "\n",
    "# inference helper for base model\n",
    "def generate_answer_base(prompt, max_new_tokens=200):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "# test\n",
    "q = 'Explain the ATT&CK technique Rc.common.'\n",
    "prompt = f\"### Instruction:\\n{q}\\n\\n### Response:\\n\"\n",
    "\n",
    "print(\"=== Base model ===\")\n",
    "print(generate_answer_base(prompt, max_new_tokens=150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T03:46:38.700286Z",
     "iopub.status.busy": "2025-08-10T03:46:38.699011Z",
     "iopub.status.idle": "2025-08-10T04:11:39.845029Z",
     "shell.execute_reply": "2025-08-10T04:11:39.844177Z",
     "shell.execute_reply.started": "2025-08-10T03:46:38.700258Z"
    },
    "id": "7_v_ut2OVSIP",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 24:49, Epoch 11/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.870500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.815900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.721100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.653400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.598400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.616800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.551500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.541500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.429300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.468400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.354200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.371000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.325500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.255100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150, training_loss=0.5735097932815552, metrics={'train_runtime': 1499.0347, 'train_samples_per_second': 1.601, 'train_steps_per_second': 0.1, 'total_flos': 5.060002450086298e+16, 'train_loss': 0.5735097932815552, 'epoch': 11.56})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10. training with 'Trainer'\n",
    "from transformers import Trainer, TrainingArguments, default_data_collator\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(DRIVE_BASE, 'ti_llm_output'),\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=150,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    optim='paged_adamw_8bit',\n",
    "    disable_tqdm=False,\n",
    "    report_to=\"none\",\n",
    "    #logging_strategy=\"steps\"\n",
    ")\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T04:12:12.976499Z",
     "iopub.status.busy": "2025-08-10T04:12:12.975774Z",
     "iopub.status.idle": "2025-08-10T04:12:13.222445Z",
     "shell.execute_reply": "2025-08-10T04:12:13.221584Z",
     "shell.execute_reply.started": "2025-08-10T04:12:12.976466Z"
    },
    "id": "JqaBVRL6WGQC",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved peft adapter files to:  /kaggle/working/ti_llm/peft_lora_ti_adapter\n"
     ]
    }
   ],
   "source": [
    "#11. save LoRA adapter and tokenizer\n",
    "\n",
    "peft_dir = os.path.join(DRIVE_BASE, 'peft_lora_ti_adapter')\n",
    "model.save_pretrained(peft_dir)\n",
    "tokenizer.save_pretrained(peft_dir)\n",
    "print('Saved peft adapter files to: ', peft_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T04:15:19.665384Z",
     "iopub.status.busy": "2025-08-10T04:15:19.664788Z",
     "iopub.status.idle": "2025-08-10T04:16:44.121111Z",
     "shell.execute_reply": "2025-08-10T04:16:44.120190Z",
     "shell.execute_reply.started": "2025-08-10T04:15:19.665359Z"
    },
    "id": "m9RtzZcyWT2h",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b1e2203faa4eab820fa56e7f2f892c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fine-tuned (LoRA) model ===\n",
      "### Instruction:\n",
      "Explain the ATT&CK technique Rc.common.\n",
      "\n",
      "### Response:\n",
      "During the boot process, macOS executes <code>source /etc/rc.common</code>, which is a shell script containing various utility functions. This file also defines routines for processing command-line arguments and for setting environment variables, such as <code>OSXShell>\n",
      "\n",
      "Adversaries can infect the <code>rc.common</code> file with shell scripts that execute during bootup as the root account. This technique can be used to persistively infect the system.\n",
      "\n",
      "### Mach-Ones\n",
      "\n",
      "There is a variant of this technique that targets the Mach-ones file <code>source /etc/mach-ones</code>. This file is similar to <code>rc\n"
     ]
    }
   ],
   "source": [
    "#12. inference: load base model + LoRA adapter and run queries\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={'': 0},\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model_base.eval()\n",
    "\n",
    "model_with_lora = PeftModel.from_pretrained(\n",
    "    model_base,\n",
    "    peft_dir,\n",
    "    device_map={'': 0}\n",
    ")\n",
    "model_with_lora.eval()\n",
    "\n",
    "# inference helper for LoRA fine-tuned model\n",
    "def generate_answer_lora(prompt, max_new_tokens=200):\n",
    "  inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n",
    "  with torch.no_grad():\n",
    "    out = model_with_lora.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "  return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "# test fine-tuned model\n",
    "q = 'Explain the ATT&CK technique Rc.common.'\n",
    "prompt = f\"### Instruction:\\n{q}\\n\\n### Response:\\n\"\n",
    "\n",
    "print(\"\\n=== Fine-tuned (LoRA) model ===\")\n",
    "print(generate_answer_lora(prompt, max_new_tokens=150))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1dkxmBQih51-wv62kVwz5W-NuH3e5yWb8",
     "timestamp": 1754621838918
    }
   ]
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

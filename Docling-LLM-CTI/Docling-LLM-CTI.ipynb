{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e2a927",
   "metadata": {},
   "source": [
    "# Docling parser, LLM enrichment, IOC extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af69b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Setup\n",
    "#!pip install -q docling pypdfium2 pillow pyarrow fastparquet\n",
    "import os, re, json, hashlib, base64, ipaddress, urllib3, sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tldextract\n",
    "\n",
    "# OpenAI api key for enrichment, make sure you have enough API budget in OpenAI for your queries\n",
    "OPENAI_API_KEY = \"YOUR_OPENAI_TOKEN\"\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee097d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: C:\\Users\\ronal\\Desktop\\LLM Labs\\data_inputs\n",
      "OUT_DIR: C:\\Users\\ronal\\Desktop\\LLM Labs\\generated\n"
     ]
    }
   ],
   "source": [
    "#2. Configuration\n",
    "DATA_DIR = Path(\"./data_inputs\")\n",
    "OUT_DIR = Path(\"./generated\")\n",
    "OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "PARSED_DIR = OUT_DIR / \"parsed\"; PARSED_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "print(\"DATA_DIR:\", DATA_DIR.resolve())\n",
    "print(\"OUT_DIR:\", OUT_DIR.resolve())\n",
    "\n",
    "# MISP url and key to push IOCs to MISP\n",
    "MISP_URL = \"https://localhost\"\n",
    "MISP_KEY = \"YOUR_MISP_TOKEN\"\n",
    "VERIFY_SSL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b11bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d068dd6279422f86ea6c6ee6a09f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install ipywidgets        # HF token for Hugging Face login: YOUR_HF_TOKEN\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ff8fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter `strict_text` has been deprecated and will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed docs: 1 -> generated\\parsed_docs.parquet (and markdown in generated\\parsed).\n",
      "texts available to IOC extractor: 1 files.\n"
     ]
    }
   ],
   "source": [
    "#3. Docling parser\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*pin_memory.*\")\n",
    "\n",
    "def sha1(s: str) -> str:\n",
    "    return hashlib.sha1(s.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
    "\n",
    "def as_plaintext_fallback(p: Path) -> str:\n",
    "    try:\n",
    "        return p.read_text(errors=\"ignore\")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def docling_available() -> bool:\n",
    "    try:\n",
    "        import docling\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "rows = []\n",
    "\n",
    "if docling_available():\n",
    "    from docling.document_converter import DocumentConverter\n",
    "    converter = DocumentConverter()         # uses defaults, includes PDF, Office, HTML, etc.\n",
    "\n",
    "    for f in sorted(DATA_DIR.rglob(\"*\")):\n",
    "        if not f.is_file():\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            result = converter.convert(f)            # returns a ConversionResult\n",
    "            doc = result.document                    # unified document abstraction\n",
    "            md = doc.export_to_markdown()            # nice for manual review\n",
    "            txt = doc.export_to_text()               # flat text for regex/LLM\n",
    "        except Exception as e:\n",
    "            # fallback for simple formats if Docling fails\n",
    "            if f.suffix.lower() in {\".txt\", \".md\", \".html\", \".htm\"}:\n",
    "                txt = as_plaintext_fallback(f)\n",
    "                md  = txt\n",
    "            else:\n",
    "                print(f\"[Docling] failed on {f.name}: {e}\", file=sys.stderr)\n",
    "                continue\n",
    "\n",
    "        # Save markdown sidecar (optional, helpful to inspect context around IOCs)\n",
    "        sidecar = (PARSED_DIR / (f.name + \".md\"))\n",
    "        sidecar.write_text(md, encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "        rows.append({\n",
    "            \"source\": f.name,\n",
    "            \"path\": str(f),\n",
    "            \"sha1\": sha1(txt[:200000]),     # content hash for dedupe/versioning\n",
    "            \"text\": txt,\n",
    "            \"markdown_path\": str(sidecar),\n",
    "            \"bytes\": f.stat().st_size\n",
    "        })\n",
    "else:\n",
    "    print(\"[Info] Docling not installed; using plaintext fallback for txt/md/html.\", file=sys.stderr)\n",
    "    for f in sorted(DATA_DIR.rglob(\"*\")):\n",
    "        if not f.is_file():\n",
    "            continue\n",
    "        if f.suffix.lower() in {\".txt\", \".md\", \".html\", \".htm\"}:\n",
    "            txt = as_plaintext_fallback(f)\n",
    "            rows.append({\n",
    "                \"source\": f.name,\n",
    "                \"path\": str(f),\n",
    "                \"sha1\": sha1(txt[:200000]),\n",
    "                \"text\": txt,\n",
    "                \"markdown_path\": \"\",\n",
    "                \"bytes\": f.stat().st_size\n",
    "            })\n",
    "\n",
    "parsed_df = pd.DataFrame(rows)\n",
    "parquet_path = OUT_DIR / \"parsed_docs.parquet\"\n",
    "parsed_df.to_parquet(parquet_path, index=False)\n",
    "print(f\"Parsed docs: {len(parsed_df)} -> {parquet_path} (and markdown in {PARSED_DIR}).\")\n",
    "\n",
    "texts = {row.source: row.text for row in parsed_df.itertuples()}\n",
    "print(f\"texts available to IOC extractor: {len(texts)} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf092fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 75 IOCs -> generated\\iocs_extracted.csv.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>type</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>group-ib-conti-threat-research-2022-en.pdf</td>\n",
       "      <td>ipv4</td>\n",
       "      <td>127.0.0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>group-ib-conti-threat-research-2022-en.pdf</td>\n",
       "      <td>ipv4</td>\n",
       "      <td>21.9.2.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>group-ib-conti-threat-research-2022-en.pdf</td>\n",
       "      <td>md5</td>\n",
       "      <td>0762764e298c369a2de8afaec5174ed9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>group-ib-conti-threat-research-2022-en.pdf</td>\n",
       "      <td>md5</td>\n",
       "      <td>1c6363248c917b9b2a0e37e547cb1bd5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>group-ib-conti-threat-research-2022-en.pdf</td>\n",
       "      <td>md5</td>\n",
       "      <td>04a5b5ecf057134a96ba9beac224c672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>group-ib-conti-threat-research-2022-en.pdf</td>\n",
       "      <td>md5</td>\n",
       "      <td>6078dbad380775d01ce9cf91cbe23d7b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>group-ib-conti-threat-research-2022-en.pdf</td>\n",
       "      <td>md5</td>\n",
       "      <td>c720441cc3603483defcad7f2476c220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>group-ib-conti-threat-research-2022-en.pdf</td>\n",
       "      <td>md5</td>\n",
       "      <td>790cfe1f9b1f7a1b8805f3c581aeb1c3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>group-ib-conti-threat-research-2022-en.pdf</td>\n",
       "      <td>md5</td>\n",
       "      <td>bcf121ba763f4a0c07113046e5103900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>group-ib-conti-threat-research-2022-en.pdf</td>\n",
       "      <td>md5</td>\n",
       "      <td>01a584f26eace00ff96f6511bab5bfee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       source  type  \\\n",
       "0  group-ib-conti-threat-research-2022-en.pdf  ipv4   \n",
       "1  group-ib-conti-threat-research-2022-en.pdf  ipv4   \n",
       "2  group-ib-conti-threat-research-2022-en.pdf   md5   \n",
       "3  group-ib-conti-threat-research-2022-en.pdf   md5   \n",
       "4  group-ib-conti-threat-research-2022-en.pdf   md5   \n",
       "5  group-ib-conti-threat-research-2022-en.pdf   md5   \n",
       "6  group-ib-conti-threat-research-2022-en.pdf   md5   \n",
       "7  group-ib-conti-threat-research-2022-en.pdf   md5   \n",
       "8  group-ib-conti-threat-research-2022-en.pdf   md5   \n",
       "9  group-ib-conti-threat-research-2022-en.pdf   md5   \n",
       "\n",
       "                              value  \n",
       "0                         127.0.0.1  \n",
       "1                        21.9.2.172  \n",
       "2  0762764e298c369a2de8afaec5174ed9  \n",
       "3  1c6363248c917b9b2a0e37e547cb1bd5  \n",
       "4  04a5b5ecf057134a96ba9beac224c672  \n",
       "5  6078dbad380775d01ce9cf91cbe23d7b  \n",
       "6  c720441cc3603483defcad7f2476c220  \n",
       "7  790cfe1f9b1f7a1b8805f3c581aeb1c3  \n",
       "8  bcf121ba763f4a0c07113046e5103900  \n",
       "9  01a584f26eace00ff96f6511bab5bfee  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4. IOC Extraction\n",
    "IOC_REGEX = {\n",
    "    \"ipv4\": re.compile(r\"\\b(?:(?:25[0-5]|2[0-4]\\d|1?\\d?\\d)\\.){3}(?:25[0-5]|2[0-4]\\d|1?\\d?\\d)\\b\"),\n",
    "    \"md5\": re.compile(r\"\\b[a-fA-F0-9]{32}\\b\"),\n",
    "    \"sha1\": re.compile(r\"\\b[a-fA-F0-9]{40}\\b\"),\n",
    "    \"sha256\": re.compile(r\"\\b[a-fA-F0-9]{64}\\b\"),\n",
    "    \"url\": re.compile(r\"\\bhttps?://[\\w\\-\\.\\/:\\?#\\[\\]@!$&'()*+,;=%]+\", re.IGNORECASE),\n",
    "    \"email\": re.compile(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\"),\n",
    "}\n",
    "\n",
    "def is_domain(token: str) -> bool:\n",
    "    ext = tldextract.extract(token)         # split a string into domain, subdomain, and suffix by using PSL(Public Suffix List)-aware validation\n",
    "    return bool(ext.domain and ext.suffix)  # true if there is a second level domain and a known public suffix\n",
    "\n",
    "def extract_domains(text: str):\n",
    "    candidates = set()\n",
    "    for token in re.findall(r\"[\\w.-]+\\.[a-zA-Z]{2,}\", text):    # find dotty tokens look like domain\n",
    "        if token.lower().startswith(\"http\"):                    # exclude http string which already has been counted\n",
    "            continue\n",
    "        if is_domain(token):                                    # to keep only strings that tldextract recognizes as a registable domain\n",
    "            candidates.add(token.lower())\n",
    "    return sorted(candidates)\n",
    "\n",
    "def extract_iocs_from_text(text: str):\n",
    "    out = {k: [] for k in [\"ipv4\", \"md5\", \"sha1\", \"sha256\", \"url\", \"email\",\"domain\"]}   # prepare an output dict for each IOC type\n",
    "    for k, rx in IOC_REGEX.items():\n",
    "        if k == \"url\":\n",
    "            out[k] = list({m.group(0).strip(').,;\"\\'') for m in rx.finditer(text)})     # trim common trailing punctuation like ).,;\"' that often clings to links in prose.\n",
    "        else:\n",
    "            out[k] = list({m.group(0) for m in rx.finditer(text)})\n",
    "    out[\"domain\"] = extract_domains(text)\n",
    "    return out\n",
    "\n",
    "def load_texts_from_dir(path: Path):\n",
    "    texts = {}\n",
    "    for p in path.glob(\"**/*\"):\n",
    "        if p.is_file() and p.suffix.lower() in {\".txt\",\".md\",\".html\",\".htm\",\".log\"}:    # load input files with these extensions\n",
    "            try:\n",
    "                texts[p.name] = p.read_text(errors=\"ignore\")\n",
    "            except Exception:\n",
    "                pass\n",
    "    return texts\n",
    "\n",
    "#texts = load_texts_from_dir(DATA_DIR)\n",
    "#print(f\"Loaded {len(texts)} text files from {DATA_DIR}.\")\n",
    "\n",
    "rows = []\n",
    "for fname, content in texts.items():\n",
    "    iocs = extract_iocs_from_text(content)\n",
    "    for t, values in iocs.items():\n",
    "        for v in values:\n",
    "            rows.append({\"source\": fname, \"type\": t, \"value\": v})\n",
    "\n",
    "ioc_df = pd.DataFrame(rows).drop_duplicates().reset_index(drop=True)\n",
    "ioc_csv = OUT_DIR / \"iocs_extracted.csv\"\n",
    "ioc_df.to_csv(ioc_csv, index=False)\n",
    "print(f\"Extracted {len(ioc_df)} IOCs -> {ioc_csv}.\")\n",
    "ioc_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2daa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planned lookups this run: 74 (from offset 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronal\\AppData\\Local\\Temp\\ipykernel_15716\\1013088523.py:151: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out_df = pd.concat([out_df, tmp], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[checkpoint] where 25 rows to generated\\vt_reputation.csv.\n",
      "[checkpoint] where 50 rows to generated\\vt_reputation.csv.\n",
      "Done. Total rows in generated\\vt_reputation.csv: 74.\n",
      "Flagged subset -> generated\\vt_flagged.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronal\\AppData\\Local\\Temp\\ipykernel_15716\\1013088523.py:167: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  flagged = out_df.fillna(0)\n"
     ]
    }
   ],
   "source": [
    "#5. check extracted IOCs reputation in VirusTotal\n",
    "import requests, time\n",
    "\n",
    "VT_API_KEY = \"YOUR_VT_TOKEN\"\n",
    "RATE_LIMIT_QPM = int(\"100\")     # queries per minute; free VT API keys are low.\n",
    "SLEEP_SEC = max(60.0 / max(1, RATE_LIMIT_QPM), 10.0)\n",
    "CHUNK_CHECKPOINT = 25           # write csv checkpoint after this many lookups\n",
    "START_OFFSET = int(\"0\")\n",
    "MAX_ITEMS = int(\"250\")          # process at most this many items this run\n",
    "VT_ONLY_NEW = True              # skip items already present in vt_reputation.csv file\n",
    "\n",
    "IOC_CSV = OUT_DIR / \"iocs_extracted.csv\"    # input\n",
    "OUT_CSV = OUT_DIR / \"vt_reputation.csv\"     # output\n",
    "\n",
    "# load IOCs from in-memory df if present, else from csv file\n",
    "if \"ioc_df\" in globals() and isinstance(ioc_df, pd.DataFrame) and not ioc_df.empty:\n",
    "    src_df = ioc_df.copy()\n",
    "else:\n",
    "    if not IOC_CSV.exists():\n",
    "        raise FileNotFoundError(f\"IOC csv not found: {IOC_CSV}. Run Part 1 script to generate it first.\")\n",
    "    src_df = pd.read_csv(IOC_CSV)\n",
    "\n",
    "# canonicalize and split\n",
    "def get_values(df, t):\n",
    "    return sorted(set(df.loc[df[\"type\"] == t, \"value\"].dropna().astype(str)))\n",
    "\n",
    "ips = get_values(src_df, \"ipv4\")\n",
    "domains = get_values(src_df, \"domain\")\n",
    "urls = get_values(src_df, \"url\")\n",
    "hashes = sorted(set(pd.concat([\n",
    "    src_df.loc[src_df[\"type\"] == \"md5\", \"value\"],\n",
    "    src_df.loc[src_df[\"type\"] == \"sha1\", \"value\"],\n",
    "    src_df.loc[src_df[\"type\"] == \"sha256\", \"value\"],\n",
    "], axis=0).dropna().astype(str)))\n",
    "\n",
    "def vt_headers():\n",
    "    return {\"x-apikey\": VT_API_KEY}\n",
    "\n",
    "def vt_url_for(kind, value):\n",
    "    if kind == \"ip\":\n",
    "        return f\"https://www.virustotal.com/api/v3/ip_addresses/{value}\"\n",
    "    if kind == \"domain\":\n",
    "        return f\"https://www.virustotal.com/api/v3/domains/{value}\"\n",
    "    if kind == \"url\":\n",
    "        url_id = base64.urlsafe_b64encode(value.encode()).decode().strip(\"=\")   # url_id is urlsafe base64 of the url, without padding\n",
    "        return f\"https://www.virustotal.com/api/v3/urls/{url_id}\"\n",
    "    if kind == \"file\":\n",
    "        return f\"https://www.virustotal.com/api/v3/files/{value}\"\n",
    "    raise ValueError(kind)\n",
    "\n",
    "def classify_kind(value, explicit_type=None):\n",
    "    if explicit_type:\n",
    "        return explicit_type\n",
    "    try:\n",
    "        ipaddress.IPv4Address(value)\n",
    "        return \"ip\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    if value.startswith(\"http://\") or value.startswith(\"https://\"):\n",
    "        return \"url\"\n",
    "    if len(value) in (32, 40, 64) and all(c in \"0123456789abcdefABCDEF\" for c in value):\n",
    "        return \"file\"\n",
    "    return \"domain\"\n",
    "\n",
    "def parse_stats(kind, vt_json):\n",
    "    stats = {\"malicious\": None, \"suspicious\": None, \"undetected\": None, \"harmless\": None}\n",
    "    rep = None\n",
    "    last_date = None\n",
    "    try:\n",
    "        attr = vt_json.get(\"data\", {}).get(\"attributes\", {})\n",
    "        s = attr.get(\"last_analysis_stats\") or {}\n",
    "        stats.update({k: s.get(k) for k in stats.keys()})\n",
    "        rep = attr.get(\"reputation\")\n",
    "        last_date = attr.get(\"last_analysis_date\") or attr.get(\"creation_date\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return stats, rep, last_date\n",
    "\n",
    "# load existing results to support resumable runs\n",
    "if OUT_CSV.exists():\n",
    "    out_df = pd.read_csv(OUT_CSV)\n",
    "    already = set(zip(out_df[\"type\"], out_df[\"value\"]))\n",
    "else:\n",
    "    out_df = pd.DataFrame(columns=[\"type\",\"value\",\"malicious\",\"suspicious\",\"undetected\",\"harmless\",\"reputation\",\"last_analysis_date\",\"http_status\",\"error\"])\n",
    "    already = set()\n",
    "\n",
    "def iter_items():       # ordered priority: file hashes -> urls -> domains -> ips\n",
    "    for kind, seq in ((\"file\", hashes), (\"url\", urls), (\"domain\", domains), (\"ip\", ips)):\n",
    "        for v in seq:\n",
    "            yield kind, v\n",
    "\n",
    "items = list(iter_items())\n",
    "\n",
    "# slice by offset/limit\n",
    "slice_items = items[START_OFFSET: START_OFFSET + MAX_ITEMS]\n",
    "\n",
    "print(f\"Planned lookups this run: {len(slice_items)} (from offset {START_OFFSET})\")\n",
    "\n",
    "rows = []\n",
    "seen = 0\n",
    "last_req_ts = 0.0\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "for kind, value in slice_items:\n",
    "    if VT_ONLY_NEW and (kind, value) in already:    # skip IOCs which are already checked\n",
    "        continue\n",
    "    elapsed = time.time() - last_req_ts\n",
    "    if elapsed < SLEEP_SEC:\n",
    "        time.sleep(SLEEP_SEC - elapsed)\n",
    "    last_req_ts = time.time()\n",
    "\n",
    "    url = vt_url_for(kind, value)\n",
    "    status = None\n",
    "    err = None\n",
    "    stats, rep, when = {}, None, None\n",
    "\n",
    "    try:\n",
    "        r = session.get(url, headers=vt_headers(), timeout=30)\n",
    "        status = r.status_code\n",
    "        if r.status_code == 200:\n",
    "            data = r.json()\n",
    "            stats, rep, when = parse_stats(kind, data)\n",
    "        elif r.status_code in (404, 400):\n",
    "            err = f\"VT status {r.status_code}\"\n",
    "        elif r.status_code == 429:\n",
    "            err = \"Rate limited (429). Increase SLEEP_SEC or lower RATE_LIMIT_QPM.\"\n",
    "        else:\n",
    "            err = f\"HTTP {r.status_code}\"\n",
    "    except Exception as e:\n",
    "        err = str(e)\n",
    "\n",
    "    row = {\n",
    "        \"type\": kind,\n",
    "        \"value\": value,\n",
    "        \"malicious\": stats.get(\"malicious\"),\n",
    "        \"suspicious\": stats.get(\"suspicious\"),\n",
    "        \"undetected\": stats.get(\"undetected\"),\n",
    "        \"harmless\": stats.get(\"harmless\"),\n",
    "        \"reputation\": rep,\n",
    "        \"last_analysis_date\": when,\n",
    "        \"http_status\": status,\n",
    "        \"error\": err\n",
    "    }\n",
    "    rows.append(row)\n",
    "    seen += 1\n",
    "\n",
    "    # periodic checkpoint\n",
    "    if seen % CHUNK_CHECKPOINT == 0:\n",
    "        tmp = pd.DataFrame(rows)\n",
    "        out_df = pd.concat([out_df, tmp], ignore_index=True)\n",
    "        out_df.drop_duplicates(subset=[\"type\",\"value\"], keep=\"last\", inplace=True)\n",
    "        out_df.to_csv(OUT_CSV, index=False)\n",
    "        print(f\"[checkpoint] where {len(out_df)} rows to {OUT_CSV}.\")\n",
    "        rows.clear()\n",
    "\n",
    "# final write\n",
    "if rows:\n",
    "    tmp = pd.DataFrame(rows)\n",
    "    out_df = pd.concat([out_df, tmp], ignore_index=True)\n",
    "    out_df.drop_duplicates(subset=[\"type\",\"value\"], keep=\"last\", inplace=True)\n",
    "    out_df.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(f\"Done. Total rows in {OUT_CSV}: {len(out_df)}.\")\n",
    "\n",
    "# Use filters to show only 'malicious >= 1' or negative reputation.\n",
    "flagged = out_df.fillna(0)\n",
    "flagged = flagged[(flagged[\"malicious\"].astype(float) >= 1) | (flagged[\"reputation\"].astype(float) < 0)]\n",
    "flagged_csv = OUT_DIR / \"vt_flagged.csv\"\n",
    "flagged.to_csv(flagged_csv, index=False)\n",
    "print(\"Flagged subset ->\", flagged_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b82a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 27 IOCs for feeding MISP -> generated\\iocs_misp.csv.\n"
     ]
    }
   ],
   "source": [
    "#6. use filter to keep only \"reputation < 0\" in the final IOC list\n",
    "VT_REPUTATION = OUT_DIR / \"vt_reputation.csv\"\n",
    "IOCS_MISP_CSV = OUT_DIR / \"iocs_misp.csv\"\n",
    "MISP_EVENT_JSON = OUT_DIR / \"misp_event_vtneg.json\"\n",
    "\n",
    "if not VT_REPUTATION.exists():\n",
    "    raise FileNotFoundError(f\"{VT_REPUTATION} not found, run the VT reputation checker first.\")\n",
    "\n",
    "df = pd.read_csv(VT_REPUTATION)\n",
    "# Keep only artifacts that exist in VT that have negative reputation\n",
    "df = df[(df[\"http_status\"] == 200) & (df[\"reputation\"].astype(float) < 0)].copy()\n",
    "\n",
    "# Normalize IOC types for MISP\n",
    "def normalize_type_and_value(row):\n",
    "    t, v = str(row[\"type\"]).lower(), str(row[\"value\"]).strip()\n",
    "    if t == \"ip\":\n",
    "        # allow only valid IPv4\n",
    "        try:\n",
    "            ipaddress.IPv4Address(v)\n",
    "            return \"ipv4\", v\n",
    "        except Exception:\n",
    "            return None, None\n",
    "    elif t == \"domain\":\n",
    "        return \"domain\", v.lower()\n",
    "    elif t == \"url\":\n",
    "        # trim trailing punctuation\n",
    "        v = v.strip(').,;\\'\"')\n",
    "        return \"url\", v\n",
    "    elif t == \"file\":\n",
    "        # decide hash type by length\n",
    "        hv = v.lower()\n",
    "        L = len(hv)\n",
    "        if L == 32 and re.fullmatch(r\"[0-9a-f]{32}\", hv):\n",
    "            return \"md5\", hv\n",
    "        if L == 40 and re.fullmatch(r\"[0-9a-f]{40}\", hv):\n",
    "            return \"sha1\", hv\n",
    "        if L == 64 and re.fullmatch(r\"[0-9a-f]{64}\", hv):\n",
    "            return \"sha256\", hv\n",
    "        return None, None\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "norm = df.apply(lambda r: normalize_type_and_value(r), axis=1, result_type=\"expand\")\n",
    "df[\"norm_type\"], df[\"norm_value\"] = norm[0], norm[1]\n",
    "df = df.dropna(subset=[\"norm_type\",\"norm_value\"]).copy()\n",
    "\n",
    "# Final minimal list for MISP\n",
    "final_iocs = df[[\"norm_type\",\"norm_value\"]].drop_duplicates().rename(\n",
    "    columns={\"norm_type\":\"type\", \"norm_value\":\"value\"}\n",
    ")\n",
    "final_iocs.to_csv(IOCS_MISP_CSV, index=False)\n",
    "print(f\"Prepared {len(final_iocs)} IOCs for feeding MISP -> {IOCS_MISP_CSV}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a40cc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM available: True\n",
      "Wrote: generated\\iocs_enriched.json\n",
      "Example: {\n",
      "  \"source\": \"group-ib-conti-threat-research-2022-en.pdf\",\n",
      "  \"type\": \"sha256\",\n",
      "  \"value\": \"904e0855772f56721cc157641a26bb7963651e5a45c3bb90764328b17081abd5\",\n",
      "  \"context\": \"This IOC likely represents a malicious payload or file associated with an attack.\",\n",
      "  \"attack_mapping\": [],\n",
      "  \"confidence\": 20,\n",
      "  \"rationale\": \"There is no external evidence to provide context or specific techniques associated with this SHA256 hash, leading to a low confidence assessment.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#7. LLM enrichment for the final IOCs\n",
    "#1) load the data already ingested before\n",
    "if 'ioc_df' not in globals() or ioc_df.empty:\n",
    "    raise RuntimeError(\"ioc_df is missing/empty. Run IOC extraction first.\")\n",
    "\n",
    "def safe_ready(p: Path):\n",
    "    try:\n",
    "        return p.read_text(errors=\"ignore\")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "texts = {}\n",
    "if DATA_DIR.exists():\n",
    "    for p in DATA_DIR.glob(\"**/*\"):\n",
    "        if p.is_file() and p.suffix.lower() in {\".txt\",\".md\",\".html\",\"htm\",\".log\"}:\n",
    "            texts[p.name] = safe_ready(p)\n",
    "\n",
    "#2) pull VT verification results to add signals to the prompt\n",
    "VT_PATH = OUT_DIR / \"vt_reputation.csv\"\n",
    "vt_df = pd.read_csv(VT_PATH) if VT_PATH.exists() else pd.DataFrame(columns=[\"type\",\"value\"])\n",
    "def vt_row_for(ioc_type, ioc_value):\n",
    "    if vt_df.empty:\n",
    "        return None\n",
    "    m = vt_df[(vt_df[\"type\"].astype(str)==ioc_type) & (vt_df[\"value\"].astype(str)==ioc_value)]\n",
    "    return m.iloc[0].to_dict() if not m.empty else None\n",
    "\n",
    "#3) build context snippets around each IOC value from its source text\n",
    "def gather_snippets(row, window=160):\n",
    "    src = str(row.get(\"source\", \"\"))\n",
    "    val = str(row.get(\"value\", \"\"))\n",
    "    text = texts.get(src, \"\")\n",
    "    if not text or not val:\n",
    "        return []\n",
    "    pattern = re.escape(val)\n",
    "    snippets = []\n",
    "    for m in re.finditer(pattern, text, flags=re.IGNORECASE):\n",
    "        start = max(0, m.start() - window)\n",
    "        end = min(len(text), m.end() + window)\n",
    "        snippets.append(text[start:end].replace(\"\\n\", \" \"))\n",
    "        if len(snippets) >= 3:\n",
    "            break\n",
    "    return snippets\n",
    "\n",
    "#4) LLM call per IOC\n",
    "USE_LLM = bool(OPENAI_API_KEY)\n",
    "print(\"LLM available:\", USE_LLM)\n",
    "\n",
    "def build_prompt(ioc_item, snippets, vt):       # build a prompt template to query in ChatGPT for IOC content enrichment\n",
    "    evidence_lines = []\n",
    "    if vt:\n",
    "        ev = {\n",
    "            \"vt_http_status\": vt.get(\"http_status\"),\n",
    "            \"vt_reputation\": vt.get(\"reputation\"),\n",
    "            \"vt_malicious\": vt.get(\"malicious\"),\n",
    "            \"vt_suspicious\": vt.get(\"suspicious\"),\n",
    "            \"vt_undetected\": vt.get(\"undetected\"),\n",
    "            \"vt_harmless\": vt.get(\"harmless\"),\n",
    "        }\n",
    "        evidence_lines.append(f\"VirusTotal: {ev}\")\n",
    "    if snippets:\n",
    "        evidence_lines.append(f\"Local context: {snippets[:2]}\")\n",
    "    return f\"\"\"\n",
    "You are a CTI analyst. Given a single IOC and evidence, infer likely context and ATT&CK techniques.\n",
    "Return STRICT JSON with keys:\n",
    "- \"context\": short one-sentence summary of what this IOC likely represents (C2, payload, phish link, scanner, etc.)\n",
    "- \"attack_mapping\": array of ATT&CK technique IDs (e.g., [\"T1071\",\"T1105\"]); include only techniques you can justify\n",
    "- \"confidence\": integer 0-100 for your overall assessment\n",
    "- \"rationale\": 1-2 sentence justification referencing the evidence\n",
    "\n",
    "IOC:\n",
    "  type: {ioc_item.get('type')}\n",
    "  value: {ioc_item.get('value')}\n",
    "Evidence:\n",
    "  {os.linesep.join(evidence_lines) if evidence_lines else \"No external evidence.\"}\n",
    "\n",
    "Constraints:\n",
    "- Base your mapping on the evidence only. If insufficient, return an empty array for \"attack_mapping\" and low confidence.\n",
    "- Use only valid ATT&CK technique IDs (Txxxx).\n",
    "- JSON only, no extra keys or text.\n",
    "\"\"\".strip()\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)     # pass api key to openai client\n",
    "\n",
    "def call_llm_json(prompt, model=\"gpt-4o-mini\", temperature=0.2):        # call model and define temperature\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            response_format={\"type\":\"json_object\"},\n",
    "            messages=[\n",
    "                {\"role\":\"system\",\"content\":\"You output strict JSON only.\"},\n",
    "                {\"role\":\"user\",\"content\":prompt}\n",
    "            ],\n",
    "        )\n",
    "        return json.loads(resp.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        return {\"context\":\"Insufficient enrichment (LLM error).\",\"attack_mapping\":[],\"confidence\":5,\"rationale\":str(e)[:200]}\n",
    "\n",
    "#5) run enrichment per IOC\n",
    "records = []\n",
    "if OPENAI_API_KEY:\n",
    "    QPS = float(os.getenv(\"LLM_QPS\", \"2\"))\n",
    "    sleep_s = max(0.0, 1.0 / max(0.1, QPS))\n",
    "    for _, item in ioc_df.iterrows():\n",
    "        snippets = gather_snippets(item)\n",
    "        vt = vt_row_for(item[\"type\"], item[\"value\"])\n",
    "        prompt = build_prompt(item, snippets, vt)\n",
    "        enriched = call_llm_json(prompt)\n",
    "        records.append({**item.to_dict(), **enriched})\n",
    "        time.sleep(sleep_s)\n",
    "else:\n",
    "    for _, item in ioc_df.iterrows():\n",
    "        records.append({\n",
    "            **item.to_dict(),\n",
    "            \"context\":\"(no LLM - pass-through)\",\n",
    "            \"attack_mapping\":[],\n",
    "            \"confidence\":0,\n",
    "            \"rationale\":\"LLM enrichment disabled.\"\n",
    "        })\n",
    "#6) save enriched output\n",
    "enriched_json = OUT_DIR / \"iocs_enriched.json\"\n",
    "with open(enriched_json, \"w\") as file:\n",
    "    json.dump(records, file, indent=2)\n",
    "print(\"Wrote:\", enriched_json)\n",
    "print(\"Example:\", json.dumps(records[48], indent=2)[:600] if records else \"(no records)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fd5d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. feed final iocs to MISP\n",
    "OUT_DIR = Path(\"./generated\"); OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "FINAL_IOCS = OUT_DIR / \"iocs_misp.csv\"               # produced by VT 200-only gate\n",
    "MISP_EVENT_JSON = OUT_DIR / \"misp_event_from_final_iocs.json\"\n",
    "SKIPPED_CSV = OUT_DIR / \"misp_skipped.csv\"\n",
    "FAILED_CSV  = OUT_DIR / \"misp_failed.csv\"\n",
    "\n",
    "if not FINAL_IOCS.exists():\n",
    "    raise FileNotFoundError(f\"{FINAL_IOCS} not found. Run the VT 200-only gate cell first.\")\n",
    "\n",
    "df = pd.read_csv(FINAL_IOCS).dropna().drop_duplicates()\n",
    "print(f\"Loaded final_iocs: {len(df)} rows\")\n",
    "\n",
    "def normalize_type_and_value(t, v):\n",
    "    t = str(t).lower().strip()\n",
    "    v = str(v).strip()\n",
    "\n",
    "    # URLs\n",
    "    if t == \"url\":\n",
    "        return {\"type\": \"url\", \"value\": v.strip(').,;\\'\"'), \"category\": \"Network activity\"}\n",
    "\n",
    "    # Domains\n",
    "    if t == \"domain\":\n",
    "        return {\"type\": \"domain\", \"value\": v.lower(), \"category\": \"Network activity\"}\n",
    "\n",
    "    # IPs\n",
    "    if t in (\"ip\",\"ipv4\",\"ip-dst\",\"ip-src\"):\n",
    "        try:\n",
    "            ipaddress.IPv4Address(v)\n",
    "            # Default to destination IP observable for blocklists; adjust if you track src instead\n",
    "            return {\"type\": \"ip-dst\", \"value\": v, \"category\": \"Network activity\"}\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    # File hashes\n",
    "    if t in (\"file\",\"md5\",\"sha1\",\"sha256\"):\n",
    "        hv = v.lower()\n",
    "        if t == \"md5\" or (len(hv) == 32 and re.fullmatch(r\"[0-9a-f]{32}\", hv)):\n",
    "            return {\"type\": \"md5\", \"value\": hv, \"category\": \"Artifacts dropped\"}\n",
    "        if t == \"sha1\" or (len(hv) == 40 and re.fullmatch(r\"[0-9a-f]{40}\", hv)):\n",
    "            return {\"type\": \"sha1\", \"value\": hv, \"category\": \"Artifacts dropped\"}\n",
    "        if t == \"sha256\" or (len(hv) == 64 and re.fullmatch(r\"[0-9a-f]{64}\", hv)):\n",
    "            return {\"type\": \"sha256\", \"value\": hv, \"category\": \"Artifacts dropped\"}\n",
    "        return None\n",
    "\n",
    "    # skip anything else (e.g., email) for this push\n",
    "    return None\n",
    "\n",
    "norm_rows, skipped = [], []\n",
    "for _, r in df.iterrows():\n",
    "    norm = normalize_type_and_value(r[\"type\"], r[\"value\"])\n",
    "    if norm:\n",
    "        norm_rows.append(norm)\n",
    "    else:\n",
    "        skipped.append({\"orig_type\": r[\"type\"], \"orig_value\": r[\"value\"], \"reason\": \"failed normalization\"})\n",
    "\n",
    "norm_df = pd.DataFrame(norm_rows).drop_duplicates()\n",
    "if skipped:\n",
    "    pd.DataFrame(skipped).to_csv(SKIPPED_CSV, index=False)\n",
    "    print(f\"Skipped during normalization: {len(skipped)} → {SKIPPED_CSV}\")\n",
    "\n",
    "print(\"Prepared for MISP (by type):\")\n",
    "print(norm_df.groupby(\"type\").size().to_string())\n",
    "\n",
    "# Build event (no attributes yet)\n",
    "if not (MISP_URL and MISP_KEY):\n",
    "    raise RuntimeError(\"Set MISP_URL and MISP_KEY env vars to push to MISP.\")\n",
    "\n",
    "session = requests.Session()\n",
    "headers = {\"Authorization\": MISP_KEY, \"Accept\": \"application/json\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "event_body = {\n",
    "    \"Event\": {\n",
    "        \"info\": \"Home-lab: final_iocs (VT 200-only) pushed to MISP\",\n",
    "        \"analysis\": 2,             # 0=initial,1=ongoing,2=completed\n",
    "        \"threat_level_id\": 2,      # 1=high,2=medium,3=low,4=undefined\n",
    "        \"date\": datetime.utcnow().strftime(\"%Y-%m-%d\"),\n",
    "        \"distribution\": 0          # 0=Your org only (safe default)\n",
    "    }\n",
    "}\n",
    "\n",
    "resp = session.post(f\"{MISP_URL}/events/add\", headers=headers, json=event_body, timeout=30, verify=False)\n",
    "if resp.status_code != 200:\n",
    "    raise RuntimeError(f\"Event creation failed: {resp.status_code} {resp.text}\")\n",
    "\n",
    "event = resp.json().get(\"Event\") or resp.json().get(\"event\") or {}\n",
    "event_id = event.get(\"id\")\n",
    "if not event_id:\n",
    "    raise RuntimeError(f\"Could not obtain event_id from response: {resp.text}\")\n",
    "\n",
    "with open(MISP_EVENT_JSON, \"w\") as f:\n",
    "    json.dump(resp.json(), f, indent=2)\n",
    "print(\"Created event id:\", event_id, \"→\", MISP_EVENT_JSON)\n",
    "\n",
    "# Add attributes one-by-one so we can see which ones fail\n",
    "failed = []\n",
    "added = 0\n",
    "for _, row in norm_df.iterrows():\n",
    "    attr = {\n",
    "        \"type\": row[\"type\"],\n",
    "        \"category\": row[\"category\"],\n",
    "        \"value\": row[\"value\"],\n",
    "        \"to_ids\": True,\n",
    "        \"distribution\": 0\n",
    "    }\n",
    "    r = session.post(f\"{MISP_URL}/attributes/add/{event_id}\", headers=headers, json={\"Attribute\": attr}, timeout=30, verify=False)\n",
    "    if r.status_code == 200:\n",
    "        added += 1\n",
    "    else:\n",
    "        failed.append({\n",
    "            \"type\": row[\"type\"],\n",
    "            \"value\": row[\"value\"],\n",
    "            \"status\": r.status_code,\n",
    "            \"body\": r.text[:300]\n",
    "        })\n",
    "\n",
    "print(f\"Attributes added: {added} / {len(norm_df)}\")\n",
    "if failed:\n",
    "    pd.DataFrame(failed).to_csv(FAILED_CSV, index=False)\n",
    "    print(f\"Some attributes failed to add: {len(failed)} → {FAILED_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bbc299e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs in: C:\\Users\\ronal\\Desktop\\LLM Labs\\generated\n",
      "- iocs_enriched.json\n",
      "- iocs_extracted.csv\n",
      "- iocs_misp.csv\n",
      "- parsed\n",
      "- parsed_docs.parquet\n",
      "- vt_flagged.csv\n",
      "- vt_reputation.csv\n"
     ]
    }
   ],
   "source": [
    "# final outputs\n",
    "print(\"Outputs in:\", OUT_DIR.resolve())\n",
    "for p in sorted(OUT_DIR.glob(\"*\")):\n",
    "    print(\"-\", p.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
